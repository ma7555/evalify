{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"home","text":""},{"location":"#evalify","title":"evalify","text":"<p>Evaluate Biometric Authentication Models Literally in\u00a0Seconds.</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#stable-release","title":"Stable release:","text":"<pre><code>pip install evalify\n</code></pre>"},{"location":"#bleeding-edge","title":"Bleeding edge:","text":"<pre><code>pip install git+https://github.com/ma7555/evalify.git\n</code></pre>"},{"location":"#used-for","title":"Used for","text":"<p>Evaluating all biometric authentication models, where the model output is a high-level embeddings known as feature vectors for visual or behaviour biometrics or d-vectors for auditory biometrics.</p>"},{"location":"#usage","title":"Usage","text":"<pre><code>import numpy as np\nfrom evalify import Experiment\n\nrng = np.random.default_rng()\nnphotos = 500\nemb_size = 32\nnclasses = 10\nX = rng.random((self.nphotos, self.emb_size))\ny = rng.integers(self.nclasses, size=self.nphotos)\n\nexperiment = Experiment()\nexperiment.run(X, y)\nexperiment.get_roc_auc()\nprint(experiment.roc_auc)\nprint(experiment.find_threshold_at_fpr(0.01))\n</code></pre>"},{"location":"#how-it-works","title":"How it works","text":"<ul> <li>When you run an experiment, evalify tries all the possible combinations between individuals for authentication based on the <code>X</code> and <code>y</code> parameters and returns the results including FPR, TPR, FNR, TNR and ROC AUC. <code>X</code> is an array of embeddings and <code>y</code> is an array of corresponding targets.</li> <li>Evalify can find the optimal threshold based on your agreed FPR and desired similarity or distance metric.</li> </ul>"},{"location":"#documentation","title":"Documentation:","text":"<ul> <li>https://ma7555.github.io/evalify/</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Blazing fast implementation for metrics calculation through optimized einstein sum and vectorized calculations.</li> <li>Many operations are dispatched to canonical BLAS, cuBLAS, or other specialized routines.</li> <li>Smart sampling options using direct indexing from pre-calculated arrays with total control over sampling strategy and sampling numbers.</li> <li>Supports most evaluation metrics:<ul> <li><code>cosine_similarity</code></li> <li><code>pearson_similarity</code></li> <li><code>cosine_distance</code></li> <li><code>euclidean_distance</code></li> <li><code>euclidean_distance_l2</code></li> <li><code>minkowski_distance</code></li> <li><code>manhattan_distance</code></li> <li><code>chebyshev_distance</code></li> </ul> </li> <li>Computation time for 4 metrics 4.2 million samples experiment is 24 seconds vs 51 minutes if looping using <code>scipy.spatial.distance</code> implemntations.</li> </ul>"},{"location":"#todo","title":"TODO","text":"<ul> <li>Safer memory allocation. I did not have issues but if you ran out of memory please manually set the <code>batch_size</code> argument.</li> </ul>"},{"location":"#contribution","title":"Contribution","text":"<ul> <li>Contributions are welcomed, and they are greatly appreciated! Every little bit helps, and credit will always be given.</li> <li>Please check CONTRIBUTING.md for guidelines.</li> </ul>"},{"location":"#citation","title":"Citation","text":"<ul> <li>If you use this software, please cite it using the metadata from CITATION.cff</li> </ul>"},{"location":"api/","title":"modules","text":"<p>Evalify main module used for creating the verification experiments.</p> <p>Creates experiments with embedding pairs to compare for face verification tasks including positive pairs, negative pairs and metrics calculations using a very optimized einstein sum. Many operations are dispatched to canonical BLAS, cuBLAS, or other specialized routines. Extremely large arrays are split into smaller batches, every batch would consume the roughly the maximum available memory.</p> <p>Typical usage example:</p> <pre><code>experiment = Experiment()\nexperiment.run(X, y)\n</code></pre>"},{"location":"api/#evalify.evalify.Experiment","title":"<code>Experiment</code>","text":"<p>Defines an experiment for evalifying.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Union[str, Sequence[str]]</code> <p>The list of metrics to use. Can be one or more of the following: <code>cosine_similarity</code>, <code>pearson_similarity</code>, <code>cosine_distance</code>, <code>euclidean_distance</code>, <code>euclidean_distance_l2</code>, <code>minkowski_distance</code>, <code>manhattan_distance</code> and <code>chebyshev_distance</code></p> <code>'cosine_similarity'</code> <code>same_class_samples</code> <code>StrOrInt</code> <ul> <li>'full': Samples all possible images within each class to create all     all possible positive pairs.</li> <li>int: Samples specific number of images for every class to create     nC2 pairs where n is passed integer.</li> </ul> <code>'full'</code> <code>different_class_samples</code> <code>StrIntSequence</code> <ul> <li>'full': Samples one image from every class with all possible pairs     of different classes. This can grow exponentially as the number     of images increase. (N, M) = (1, \"full\")</li> <li>'minimal': Samples one image from every class with one image of     all other classes. (N, M) = (1, 1). (Default)</li> <li>int: Samples one image from every class with provided number of     images of every other class.</li> <li>tuple or list: (N, M) Samples N images from every class with M images of     every other class.</li> </ul> <code>'minimal'</code> <code>seed</code> <code>Optional[int]</code> <p>Optional random seed for reproducibility.</p> <code>None</code> Notes <ul> <li><code>same_class_samples</code>:     If the provided number is greater than the achievable for the class,     the maximum possible combinations are used.</li> <li><code>different_class_samples</code>:     If the provided number is greater than the achievable for the class,     the maximum possible combinations are used. (N, M) can also be     ('full', 'full') but this will calculate all possible combinations     between all posibile negative samples. If the dataset is not small     this will probably result in an extremely large array!.</li> </ul> Source code in <code>evalify/evalify.py</code> <pre><code>class Experiment:\n    \"\"\"Defines an experiment for evalifying.\n\n    Args:\n        metrics: The list of metrics to use. Can be one or more of the following:\n            `cosine_similarity`, `pearson_similarity`, `cosine_distance`,\n            `euclidean_distance`, `euclidean_distance_l2`, `minkowski_distance`,\n            `manhattan_distance` and `chebyshev_distance`\n        same_class_samples:\n            - 'full': Samples all possible images within each class to create all\n                all possible positive pairs.\n            -  int: Samples specific number of images for every class to create\n                nC2 pairs where n is passed integer.\n        different_class_samples:\n            - 'full': Samples one image from every class with all possible pairs\n                of different classes. This can grow exponentially as the number\n                of images increase. (N, M) = (1, \"full\")\n            - 'minimal': Samples one image from every class with one image of\n                all other classes. (N, M) = (1, 1). (Default)\n            - int: Samples one image from every class with provided number of\n                images of every other class.\n            - tuple or list: (N, M) Samples N images from every class with M images of\n                every other class.\n        seed: Optional random seed for reproducibility.\n\n\n    Notes:\n        - `same_class_samples`:\n            If the provided number is greater than the achievable for the class,\n            the maximum possible combinations are used.\n        - `different_class_samples`:\n            If the provided number is greater than the achievable for the class,\n            the maximum possible combinations are used. (N, M) can also be\n            ('full', 'full') but this will calculate all possible combinations\n            between all posibile negative samples. If the dataset is not small\n            this will probably result in an extremely large array!.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        metrics: Union[str, Sequence[str]] = \"cosine_similarity\",\n        same_class_samples: StrOrInt = \"full\",\n        different_class_samples: StrIntSequence = \"minimal\",\n        seed: Optional[int] = None,\n    ) -&gt; None:\n        self.experiment_success = False\n        self.cached_predicted_as_similarity = {}\n        self.metrics = (metrics,) if isinstance(metrics, str) else metrics\n        self.same_class_samples = same_class_samples\n        self.different_class_samples = different_class_samples\n        self.seed = seed\n\n    def __call__(self, *args: Any, **kwds: Any) -&gt; Any:\n        return self.run(*args, **kwds)\n\n    @staticmethod\n    def _validate_args(\n        metrics: Sequence[str],\n        same_class_samples: StrOrInt,\n        different_class_samples: StrIntSequence,\n        batch_size: Optional[StrOrInt],\n        p,\n    ) -&gt; None:\n        \"\"\"Validates passed arguments to Experiment.run() method.\"\"\"\n        if same_class_samples != \"full\" and not isinstance(same_class_samples, int):\n            msg = (\n                \"`same_class_samples` argument must be one of 'full' or an integer \"\n                f\"Received: same_class_samples={same_class_samples}\"\n            )\n            raise ValueError(\n                msg,\n            )\n\n        if different_class_samples not in (\"full\", \"minimal\"):\n            if not isinstance(different_class_samples, (int, list, tuple)):\n                msg = (\n                    \"`different_class_samples` argument must be one of 'full', \"\n                    \"'minimal', an integer, a list or tuple of integers or keyword \"\n                    \"'full'.\"\n                    f\"Received: different_class_samples={different_class_samples}.\"\n                )\n                raise ValueError(\n                    msg,\n                )\n            if isinstance(different_class_samples, (list, tuple)) and (\n                not (\n                    all(\n                        isinstance(i, int) or i == \"full\"\n                        for i in different_class_samples\n                    )\n                )\n                or (len(different_class_samples)) != 2\n            ):\n                msg = (\n                    \"When passing `different_class_samples` as a tuple or list, \"\n                    \"elements must be exactly two of integer type or keyword 'full' \"\n                    \"(N, M). \"\n                    f\"Received: different_class_samples={different_class_samples}.\"\n                )\n                raise ValueError(\n                    msg,\n                )\n\n        if (\n            batch_size != \"best\"\n            and not isinstance(batch_size, int)\n            and batch_size is not None\n        ):\n            msg = (\n                '`batch_size` argument must be either \"best\" or of type integer '\n                f\"Received: batch_size={batch_size} with type {type(batch_size)}.\"\n            )\n            raise ValueError(\n                msg,\n            )\n\n        if any(metric not in metrics_caller for metric in metrics):\n            msg = (\n                f\"`metric` argument must be one of {tuple(metrics_caller.keys())} \"\n                f\"Received: metric={metrics}\"\n            )\n            raise ValueError(\n                msg,\n            )\n\n        if p &lt; 1:\n            msg = f\"`p` must be an int and at least 1. Received: p={p}\"\n            raise ValueError(msg)\n\n    def _get_pairs(\n        self,\n        y,\n        same_class_samples,\n        different_class_samples,\n        target,\n    ) -&gt; List[Tuple]:\n        \"\"\"Generates experiment pairs.\"\"\"\n        same_ixs_full = np.argwhere(y == target).ravel()\n        if isinstance(same_class_samples, int):\n            same_class_samples = min(len(same_ixs_full), same_class_samples)\n            same_ixs = self.rng.choice(same_ixs_full, same_class_samples)\n        elif same_class_samples == \"full\":\n            same_ixs = same_ixs_full\n        same_pairs = itertools.combinations(same_ixs, 2)\n        same_pairs = [(a, b, target, target, 1) for a, b in same_pairs]\n\n        different_ixs = np.argwhere(y != target).ravel()\n        diff_df = pd.DataFrame(\n            data={\"sample_idx\": different_ixs, \"target\": y[different_ixs]},\n        )\n\n        diff_df = diff_df.sample(frac=1, random_state=self.seed)\n        if different_class_samples in [\"full\", \"minimal\"] or isinstance(\n            different_class_samples,\n            int,\n        ):\n            N = 1\n            if different_class_samples == \"minimal\":\n                diff_df = diff_df.drop_duplicates(subset=[\"target\"])\n        else:\n            N, M = different_class_samples\n            N = len(same_ixs_full) if N == \"full\" else min(N, len(same_ixs_full))\n            if M != \"full\":\n                diff_df = (\n                    diff_df.groupby(\"target\")\n                    .apply(lambda x: x[:M], include_groups=False)\n                    .droplevel(0)\n                )\n\n        different_ixs = diff_df.sample_idx.to_numpy()\n\n        different_pairs = itertools.product(\n            self.rng.choice(same_ixs_full, N, replace=False),\n            different_ixs,\n        )\n        different_pairs = [(a, b, target, y[b], 0) for a, b in different_pairs if a &lt; b]\n\n        return same_pairs + different_pairs\n\n    def run(\n        self,\n        X: np.ndarray,\n        y: np.ndarray,\n        batch_size: Optional[StrOrInt] = \"best\",\n        shuffle: bool = False,\n        return_embeddings: bool = False,\n        p: int = 3,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Runs an experiment for face verification\n        Args:\n            X: Embeddings array\n            y: Targets for X as integers\n            batch_size:\n                - 'best': Let the program decide based on available memory such that\n                    every batch will fit into the available memory. (Default)\n                - int: Manually decide the batch_size.\n                - None: No batching. All experiment and intermediate results must fit\n                    entirely into memory or a MemoryError will be raised.\n            shuffle: Shuffle the returned experiment dataframe. Default: False.\n            return_embeddings: Whether to return the embeddings instead of indexes.\n                Default: False\n            p:\n                The order of the norm of the difference. Should be `p &gt;= 1`, Only valid\n                with minkowski_distance as a metric. Default = 3.\n\n        Returns:\n            pandas.DataFrame: A DataFrame representing the experiment results.\n\n        Raises:\n            ValueError: An error occurred with the provided arguments.\n\n        \"\"\"\n        self._validate_args(\n            self.metrics,\n            self.same_class_samples,\n            self.different_class_samples,\n            batch_size,\n            p,\n        )\n        X, y = _validate_vectors(X, y)\n        all_targets = np.unique(y)\n        all_pairs = []\n        metric_fns = list(map(metrics_caller.get, self.metrics))\n        self.rng = np.random.default_rng(self.seed)\n        for target in all_targets:\n            all_pairs += self._get_pairs(\n                y,\n                self.same_class_samples,\n                self.different_class_samples,\n                target,\n            )\n\n        self.df = pd.DataFrame(\n            data=all_pairs,\n            columns=[\"emb_a\", \"emb_b\", \"target_a\", \"target_b\", \"target\"],\n        )\n        experiment_size = len(self.df)\n        if shuffle:\n            self.df = self.df.sample(frac=1, random_state=self.seed)\n        if batch_size == \"best\":\n            batch_size = calculate_best_batch_size(X)\n        elif batch_size is None:\n            batch_size = experiment_size\n        kwargs = {}\n        if any(metric in METRICS_NEED_NORM for metric in self.metrics):\n            kwargs[\"norms\"] = np.linalg.norm(X, axis=1)\n        if any(metric in METRICS_NEED_ORDER for metric in self.metrics):\n            kwargs[\"p\"] = p\n\n        emb_a = self.df.emb_a.to_numpy()\n        emb_b = self.df.emb_b.to_numpy()\n\n        emb_a_s = np.array_split(emb_a, np.ceil(experiment_size / batch_size))\n        emb_b_s = np.array_split(emb_b, np.ceil(experiment_size / batch_size))\n\n        for metric, metric_fn in zip(self.metrics, metric_fns):\n            self.df[metric] = np.hstack(\n                [metric_fn(X, i, j, **kwargs) for i, j in zip(emb_a_s, emb_b_s)],\n            )\n        if return_embeddings:\n            self.df[\"emb_a\"] = X[emb_a].tolist()\n            self.df[\"emb_b\"] = X[emb_b].tolist()\n\n        self.experiment_success = True\n        return self.df\n\n    def find_optimal_cutoff(self) -&gt; dict:\n        \"\"\"Finds the optimal cutoff threshold for each metric based on the ROC curve.\n\n        This function calculates the optimal threshold for each metric by finding the\n        point on the Receiver Operating Characteristic (ROC) curve where the difference\n        between the True Positive Rate (TPR) and the False Positive Rate (FPR) is\n        minimized.\n\n        Returns:\n            dict: A dictionary with metrics as keys and their corresponding optimal\n            threshold as values.\n        \"\"\"\n\n        self.check_experiment_run()\n        self.optimal_cutoff = {}\n        for metric in self.metrics:\n            fpr, tpr, threshold = roc_curve(self.df[\"target\"], self.df[metric])\n            i = np.arange(len(tpr))\n            roc = pd.DataFrame(\n                {\n                    \"tf\": pd.Series(tpr - (1 - fpr), index=i),\n                    \"threshold\": pd.Series(threshold, index=i),\n                },\n            )\n            roc_t = roc.iloc[(roc.tf - 0).abs().argsort()[:1]]\n            self.optimal_cutoff[metric] = roc_t[\"threshold\"].item()\n        return self.optimal_cutoff\n\n    def threshold_at_fpr(self, fpr: float) -&gt; dict:\n        \"\"\"Find the threshold at a specified False Positive Rate (FPR) for each metric.\n\n        The function calculates the threshold at the specified FPR for each metric\n        by using the Receiver Operating Characteristic (ROC) curve. If the desired\n        FPR is 0 or 1, or no exact match is found, the closest thresholds are used.\n\n        Args:\n            fpr (float): Desired False Positive Rate. Must be between 0 and 1.\n\n        Returns:\n            dict: A dictionary where keys are the metrics and values are dictionaries\n            containing FPR, TPR, and threshold at the specified FPR.\n\n        Raises:\n            ValueError: If the provided `fpr` is not between 0 and 1.\n        \"\"\"\n\n        self.check_experiment_run()\n        if not 0 &lt;= fpr &lt;= 1:\n            msg = \"`fpr` must be between 0 and 1. \" f\"Received wanted_fpr={fpr}\"\n            raise ValueError(\n                msg,\n            )\n        threshold_at_fpr = {}\n        for metric in self.metrics:\n            predicted = self.predicted_as_similarity(metric)\n            FPR, TPR, thresholds = roc_curve(\n                self.df[\"target\"],\n                predicted,\n                drop_intermediate=False,\n            )\n            df_fpr_tpr = pd.DataFrame({\"FPR\": FPR, \"TPR\": TPR, \"threshold\": thresholds})\n            ix_left = np.searchsorted(df_fpr_tpr[\"FPR\"], fpr, side=\"left\")\n            ix_right = np.searchsorted(df_fpr_tpr[\"FPR\"], fpr, side=\"right\")\n\n            if fpr == 0:\n                best = df_fpr_tpr.iloc[ix_right]\n            elif fpr == 1 or ix_left == ix_right:\n                best = df_fpr_tpr.iloc[ix_left]\n            else:\n                best = (\n                    df_fpr_tpr.iloc[ix_left]\n                    if abs(df_fpr_tpr.iloc[ix_left].FPR - fpr)\n                    &lt; abs(df_fpr_tpr.iloc[ix_right].FPR - fpr)\n                    else df_fpr_tpr.iloc[ix_right]\n                )\n            best = best.to_dict()\n            if metric in REVERSE_DISTANCE_TO_SIMILARITY:\n                best[\"threshold\"] = REVERSE_DISTANCE_TO_SIMILARITY.get(metric)(\n                    best[\"threshold\"],\n                )\n            threshold_at_fpr[metric] = best\n        return threshold_at_fpr\n\n    def get_binary_prediction(self, metric: str, threshold: float) -&gt; pd.Series:\n        \"\"\"Binary classification prediction based on the given metric and threshold.\n\n        Args:\n            metric: Metric name for the desired prediction.\n            threshold: Cut off threshold.\n\n        Returns:\n            pd.Series: Binary predictions.\n\n        \"\"\"\n        return (\n            self.df[metric].apply(lambda x: 1 if x &lt; threshold else 0)\n            if metric in DISTANCE_TO_SIMILARITY\n            else self.df[metric].apply(lambda x: 1 if x &gt; threshold else 0)\n        )\n\n    def evaluate_at_threshold(self, threshold: float, metric: str) -&gt; dict:\n        \"\"\"Evaluate performance at specific threshold\n        Args:\n            threshold: Cut-off threshold.\n            metric: Metric to use.\n\n        Returns:\n            dict: A dict ontaining all evaluation metrics.\n\n        \"\"\"\n        self.metrics_evaluation = {}\n        self.check_experiment_run(metric)\n        for metric in self.metrics:\n            predicted = self.get_binary_prediction(metric, threshold)\n            cm = confusion_matrix(self.df[\"target\"], predicted)\n            tn, fp, fn, tp = cm.ravel()\n            TPR = tp / (tp + fn)  # recall / true positive rate\n            TNR = tn / (tn + fp)  # true negative rate\n            PPV = tp / (tp + fp)  # precision / positive predicted value\n            NPV = tn / (tn + fn)  # negative predictive value\n            FPR = fp / (fp + tn)  # false positive rate\n            FNR = 1 - TPR  # false negative rate\n            FDR = 1 - PPV  # false discovery rate\n            FOR = 1 - NPV  # false omission rate\n            F1 = 2 * (PPV * TPR) / (PPV + TPR)\n\n            evaluation = {\n                \"TPR\": TPR,\n                \"TNR\": TNR,\n                \"PPV\": PPV,\n                \"NPV\": NPV,\n                \"FPR\": FPR,\n                \"FNR\": FNR,\n                \"FDR\": FDR,\n                \"FOR\": FOR,\n                \"F1\": F1,\n            }\n\n        return evaluation\n\n    def check_experiment_run(self, metric: Optional[str] = None) -&gt; bool:\n        caller = sys._getframe().f_back.f_code.co_name\n        if not self.experiment_success:\n            msg = (\n                f\"`{caller}` function can only be run after running \"\n                \"`run_experiment`.\"\n            )\n            raise NotImplementedError(\n                msg,\n            )\n        if metric is not None and metric not in self.metrics:\n            msg = (\n                f\"`{caller}` function can only be called with `metric` from \"\n                f\"{self.metrics} which were used while running the experiment\"\n            )\n            raise ValueError(\n                msg,\n            )\n        return True\n\n    def roc_auc(self) -&gt; OrderedDict:\n        \"\"\"Find ROC AUC for all the metrics used.\n\n        Returns:\n            OrderedDict: An OrderedDict with AUC for all metrics.\n\n        \"\"\"\n        self.check_experiment_run()\n        self.roc_auc = {}\n        for metric in self.metrics:\n            predicted = self.predicted_as_similarity(metric)\n            fpr, tpr, thresholds = roc_curve(\n                self.df[\"target\"],\n                predicted,\n                drop_intermediate=False,\n            )\n            self.roc_auc[metric] = auc(fpr, tpr).item()\n        self.roc_auc = OrderedDict(\n            sorted(self.roc_auc.items(), key=lambda x: x[1], reverse=True),\n        )\n        return self.roc_auc\n\n    def predicted_as_similarity(self, metric: str) -&gt; pd.Series:\n        \"\"\"Convert distance metrics to a similarity measure.\n\n        Args:\n            metric: distance metric to convert to similarity. If a similarity metric is\n                passed, It gets returned unchanged.\n\n        Returns:\n            pd.Series: Converted distance to similarity.\n\n        \"\"\"\n        predicted = self.df[metric]\n        if metric in DISTANCE_TO_SIMILARITY:\n            predicted = (\n                self.cached_predicted_as_similarity[metric]\n                if metric in self.cached_predicted_as_similarity\n                else DISTANCE_TO_SIMILARITY.get(metric)(predicted)\n            )\n            self.cached_predicted_as_similarity[metric] = predicted\n        return predicted\n\n    def eer(self) -&gt; OrderedDict:\n        \"\"\"Calculates the Equal Error Rate (EER) for each metric.\n\n        Returns:\n            OrderedDict: A dictionary containing the EER value and threshold for each\n            metric.\n                The metrics are sorted in ascending order based on the EER values.\n                Example: {'metric1': {'EER': 0.123, 'threshold': 0.456},\n                        ...}\n\n        \"\"\"\n        self.check_experiment_run()\n        self.eer = {}\n        for metric in self.metrics:\n            predicted = self.predicted_as_similarity(metric)\n            actual = self.df[\"target\"]\n\n            fpr, tpr, thresholds = roc_curve(\n                actual,\n                predicted,\n                pos_label=1,\n                drop_intermediate=False,\n            )\n            fnr = 1 - tpr\n            eer_threshold = thresholds[np.nanargmin(np.absolute(fnr - fpr))].item()\n            eer_1 = fpr[np.nanargmin(np.absolute(fnr - fpr))].item()\n            eer_2 = fnr[np.nanargmin(np.absolute(fnr - fpr))].item()\n            if metric in REVERSE_DISTANCE_TO_SIMILARITY:\n                eer_threshold = REVERSE_DISTANCE_TO_SIMILARITY.get(metric)(\n                    eer_threshold,\n                )\n\n            self.eer[metric] = {\"EER\": (eer_1 + eer_2) / 2, \"threshold\": eer_threshold}\n        self.eer = OrderedDict(\n            sorted(self.eer.items(), key=lambda x: x[1][\"EER\"], reverse=False),\n        )\n\n        return self.eer\n\n    def tar_at_far(self, far_values: List[float]) -&gt; OrderedDict:\n        \"\"\"Calculates TAR at specified FAR values for each metric.\n\n        Args:\n            far_values (List[float]): A list of False Accept Rates (FAR) to get TAR\n                values for.\n\n        Returns:\n            OrderedDict: A dictionary with keys as metrics and values as dictionaries\n            of FAR:TAR pairs.\n\n        Raises:\n            ValueError: If any FAR in far_values is not between 0 and 1.\n        \"\"\"\n        if isinstance(far_values, (float, int)):\n            far_values = [float(far_values)]\n\n        if not all(0 &lt;= far &lt;= 1 for far in far_values):\n            raise ValueError(\"All FAR values must be between 0 and 1.\")\n\n        self.check_experiment_run()\n        tar_at_far_results = {}\n\n        for metric in self.metrics:\n            predicted = self.predicted_as_similarity(metric)\n            fpr, tpr, _ = roc_curve(self.df[\"target\"], predicted, pos_label=1)\n\n            tar_values = {}\n            for far in far_values:\n                idx = np.searchsorted(fpr, far, side=\"right\") - 1\n                idx = max(0, min(idx, len(fpr) - 1))  # Ensure idx is within bounds\n                tar_values[far] = tpr[idx].item()\n\n            tar_at_far_results[metric] = tar_values\n\n        self.tar_at_far_results = OrderedDict(\n            sorted(tar_at_far_results.items(), key=lambda x: list(x[1].keys())[0])\n        )\n\n        return self.tar_at_far_results\n</code></pre>"},{"location":"api/#evalify.evalify.Experiment.eer","title":"<code>eer()</code>","text":"<p>Calculates the Equal Error Rate (EER) for each metric.</p> <p>Returns:</p> Name Type Description <code>OrderedDict</code> <code>OrderedDict</code> <p>A dictionary containing the EER value and threshold for each</p> <code>OrderedDict</code> <p>metric. The metrics are sorted in ascending order based on the EER values. Example: {'metric1': {'EER': 0.123, 'threshold': 0.456},         ...}</p> Source code in <code>evalify/evalify.py</code> <pre><code>def eer(self) -&gt; OrderedDict:\n    \"\"\"Calculates the Equal Error Rate (EER) for each metric.\n\n    Returns:\n        OrderedDict: A dictionary containing the EER value and threshold for each\n        metric.\n            The metrics are sorted in ascending order based on the EER values.\n            Example: {'metric1': {'EER': 0.123, 'threshold': 0.456},\n                    ...}\n\n    \"\"\"\n    self.check_experiment_run()\n    self.eer = {}\n    for metric in self.metrics:\n        predicted = self.predicted_as_similarity(metric)\n        actual = self.df[\"target\"]\n\n        fpr, tpr, thresholds = roc_curve(\n            actual,\n            predicted,\n            pos_label=1,\n            drop_intermediate=False,\n        )\n        fnr = 1 - tpr\n        eer_threshold = thresholds[np.nanargmin(np.absolute(fnr - fpr))].item()\n        eer_1 = fpr[np.nanargmin(np.absolute(fnr - fpr))].item()\n        eer_2 = fnr[np.nanargmin(np.absolute(fnr - fpr))].item()\n        if metric in REVERSE_DISTANCE_TO_SIMILARITY:\n            eer_threshold = REVERSE_DISTANCE_TO_SIMILARITY.get(metric)(\n                eer_threshold,\n            )\n\n        self.eer[metric] = {\"EER\": (eer_1 + eer_2) / 2, \"threshold\": eer_threshold}\n    self.eer = OrderedDict(\n        sorted(self.eer.items(), key=lambda x: x[1][\"EER\"], reverse=False),\n    )\n\n    return self.eer\n</code></pre>"},{"location":"api/#evalify.evalify.Experiment.evaluate_at_threshold","title":"<code>evaluate_at_threshold(threshold, metric)</code>","text":"<p>Evaluate performance at specific threshold Args:     threshold: Cut-off threshold.     metric: Metric to use.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dict ontaining all evaluation metrics.</p> Source code in <code>evalify/evalify.py</code> <pre><code>def evaluate_at_threshold(self, threshold: float, metric: str) -&gt; dict:\n    \"\"\"Evaluate performance at specific threshold\n    Args:\n        threshold: Cut-off threshold.\n        metric: Metric to use.\n\n    Returns:\n        dict: A dict ontaining all evaluation metrics.\n\n    \"\"\"\n    self.metrics_evaluation = {}\n    self.check_experiment_run(metric)\n    for metric in self.metrics:\n        predicted = self.get_binary_prediction(metric, threshold)\n        cm = confusion_matrix(self.df[\"target\"], predicted)\n        tn, fp, fn, tp = cm.ravel()\n        TPR = tp / (tp + fn)  # recall / true positive rate\n        TNR = tn / (tn + fp)  # true negative rate\n        PPV = tp / (tp + fp)  # precision / positive predicted value\n        NPV = tn / (tn + fn)  # negative predictive value\n        FPR = fp / (fp + tn)  # false positive rate\n        FNR = 1 - TPR  # false negative rate\n        FDR = 1 - PPV  # false discovery rate\n        FOR = 1 - NPV  # false omission rate\n        F1 = 2 * (PPV * TPR) / (PPV + TPR)\n\n        evaluation = {\n            \"TPR\": TPR,\n            \"TNR\": TNR,\n            \"PPV\": PPV,\n            \"NPV\": NPV,\n            \"FPR\": FPR,\n            \"FNR\": FNR,\n            \"FDR\": FDR,\n            \"FOR\": FOR,\n            \"F1\": F1,\n        }\n\n    return evaluation\n</code></pre>"},{"location":"api/#evalify.evalify.Experiment.find_optimal_cutoff","title":"<code>find_optimal_cutoff()</code>","text":"<p>Finds the optimal cutoff threshold for each metric based on the ROC curve.</p> <p>This function calculates the optimal threshold for each metric by finding the point on the Receiver Operating Characteristic (ROC) curve where the difference between the True Positive Rate (TPR) and the False Positive Rate (FPR) is minimized.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary with metrics as keys and their corresponding optimal</p> <code>dict</code> <p>threshold as values.</p> Source code in <code>evalify/evalify.py</code> <pre><code>def find_optimal_cutoff(self) -&gt; dict:\n    \"\"\"Finds the optimal cutoff threshold for each metric based on the ROC curve.\n\n    This function calculates the optimal threshold for each metric by finding the\n    point on the Receiver Operating Characteristic (ROC) curve where the difference\n    between the True Positive Rate (TPR) and the False Positive Rate (FPR) is\n    minimized.\n\n    Returns:\n        dict: A dictionary with metrics as keys and their corresponding optimal\n        threshold as values.\n    \"\"\"\n\n    self.check_experiment_run()\n    self.optimal_cutoff = {}\n    for metric in self.metrics:\n        fpr, tpr, threshold = roc_curve(self.df[\"target\"], self.df[metric])\n        i = np.arange(len(tpr))\n        roc = pd.DataFrame(\n            {\n                \"tf\": pd.Series(tpr - (1 - fpr), index=i),\n                \"threshold\": pd.Series(threshold, index=i),\n            },\n        )\n        roc_t = roc.iloc[(roc.tf - 0).abs().argsort()[:1]]\n        self.optimal_cutoff[metric] = roc_t[\"threshold\"].item()\n    return self.optimal_cutoff\n</code></pre>"},{"location":"api/#evalify.evalify.Experiment.get_binary_prediction","title":"<code>get_binary_prediction(metric, threshold)</code>","text":"<p>Binary classification prediction based on the given metric and threshold.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>str</code> <p>Metric name for the desired prediction.</p> required <code>threshold</code> <code>float</code> <p>Cut off threshold.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Binary predictions.</p> Source code in <code>evalify/evalify.py</code> <pre><code>def get_binary_prediction(self, metric: str, threshold: float) -&gt; pd.Series:\n    \"\"\"Binary classification prediction based on the given metric and threshold.\n\n    Args:\n        metric: Metric name for the desired prediction.\n        threshold: Cut off threshold.\n\n    Returns:\n        pd.Series: Binary predictions.\n\n    \"\"\"\n    return (\n        self.df[metric].apply(lambda x: 1 if x &lt; threshold else 0)\n        if metric in DISTANCE_TO_SIMILARITY\n        else self.df[metric].apply(lambda x: 1 if x &gt; threshold else 0)\n    )\n</code></pre>"},{"location":"api/#evalify.evalify.Experiment.predicted_as_similarity","title":"<code>predicted_as_similarity(metric)</code>","text":"<p>Convert distance metrics to a similarity measure.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>str</code> <p>distance metric to convert to similarity. If a similarity metric is passed, It gets returned unchanged.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Converted distance to similarity.</p> Source code in <code>evalify/evalify.py</code> <pre><code>def predicted_as_similarity(self, metric: str) -&gt; pd.Series:\n    \"\"\"Convert distance metrics to a similarity measure.\n\n    Args:\n        metric: distance metric to convert to similarity. If a similarity metric is\n            passed, It gets returned unchanged.\n\n    Returns:\n        pd.Series: Converted distance to similarity.\n\n    \"\"\"\n    predicted = self.df[metric]\n    if metric in DISTANCE_TO_SIMILARITY:\n        predicted = (\n            self.cached_predicted_as_similarity[metric]\n            if metric in self.cached_predicted_as_similarity\n            else DISTANCE_TO_SIMILARITY.get(metric)(predicted)\n        )\n        self.cached_predicted_as_similarity[metric] = predicted\n    return predicted\n</code></pre>"},{"location":"api/#evalify.evalify.Experiment.roc_auc","title":"<code>roc_auc()</code>","text":"<p>Find ROC AUC for all the metrics used.</p> <p>Returns:</p> Name Type Description <code>OrderedDict</code> <code>OrderedDict</code> <p>An OrderedDict with AUC for all metrics.</p> Source code in <code>evalify/evalify.py</code> <pre><code>def roc_auc(self) -&gt; OrderedDict:\n    \"\"\"Find ROC AUC for all the metrics used.\n\n    Returns:\n        OrderedDict: An OrderedDict with AUC for all metrics.\n\n    \"\"\"\n    self.check_experiment_run()\n    self.roc_auc = {}\n    for metric in self.metrics:\n        predicted = self.predicted_as_similarity(metric)\n        fpr, tpr, thresholds = roc_curve(\n            self.df[\"target\"],\n            predicted,\n            drop_intermediate=False,\n        )\n        self.roc_auc[metric] = auc(fpr, tpr).item()\n    self.roc_auc = OrderedDict(\n        sorted(self.roc_auc.items(), key=lambda x: x[1], reverse=True),\n    )\n    return self.roc_auc\n</code></pre>"},{"location":"api/#evalify.evalify.Experiment.run","title":"<code>run(X, y, batch_size='best', shuffle=False, return_embeddings=False, p=3)</code>","text":"<p>Runs an experiment for face verification Args:     X: Embeddings array     y: Targets for X as integers     batch_size:         - 'best': Let the program decide based on available memory such that             every batch will fit into the available memory. (Default)         - int: Manually decide the batch_size.         - None: No batching. All experiment and intermediate results must fit             entirely into memory or a MemoryError will be raised.     shuffle: Shuffle the returned experiment dataframe. Default: False.     return_embeddings: Whether to return the embeddings instead of indexes.         Default: False     p:         The order of the norm of the difference. Should be <code>p &gt;= 1</code>, Only valid         with minkowski_distance as a metric. Default = 3.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: A DataFrame representing the experiment results.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>An error occurred with the provided arguments.</p> Source code in <code>evalify/evalify.py</code> <pre><code>def run(\n    self,\n    X: np.ndarray,\n    y: np.ndarray,\n    batch_size: Optional[StrOrInt] = \"best\",\n    shuffle: bool = False,\n    return_embeddings: bool = False,\n    p: int = 3,\n) -&gt; pd.DataFrame:\n    \"\"\"Runs an experiment for face verification\n    Args:\n        X: Embeddings array\n        y: Targets for X as integers\n        batch_size:\n            - 'best': Let the program decide based on available memory such that\n                every batch will fit into the available memory. (Default)\n            - int: Manually decide the batch_size.\n            - None: No batching. All experiment and intermediate results must fit\n                entirely into memory or a MemoryError will be raised.\n        shuffle: Shuffle the returned experiment dataframe. Default: False.\n        return_embeddings: Whether to return the embeddings instead of indexes.\n            Default: False\n        p:\n            The order of the norm of the difference. Should be `p &gt;= 1`, Only valid\n            with minkowski_distance as a metric. Default = 3.\n\n    Returns:\n        pandas.DataFrame: A DataFrame representing the experiment results.\n\n    Raises:\n        ValueError: An error occurred with the provided arguments.\n\n    \"\"\"\n    self._validate_args(\n        self.metrics,\n        self.same_class_samples,\n        self.different_class_samples,\n        batch_size,\n        p,\n    )\n    X, y = _validate_vectors(X, y)\n    all_targets = np.unique(y)\n    all_pairs = []\n    metric_fns = list(map(metrics_caller.get, self.metrics))\n    self.rng = np.random.default_rng(self.seed)\n    for target in all_targets:\n        all_pairs += self._get_pairs(\n            y,\n            self.same_class_samples,\n            self.different_class_samples,\n            target,\n        )\n\n    self.df = pd.DataFrame(\n        data=all_pairs,\n        columns=[\"emb_a\", \"emb_b\", \"target_a\", \"target_b\", \"target\"],\n    )\n    experiment_size = len(self.df)\n    if shuffle:\n        self.df = self.df.sample(frac=1, random_state=self.seed)\n    if batch_size == \"best\":\n        batch_size = calculate_best_batch_size(X)\n    elif batch_size is None:\n        batch_size = experiment_size\n    kwargs = {}\n    if any(metric in METRICS_NEED_NORM for metric in self.metrics):\n        kwargs[\"norms\"] = np.linalg.norm(X, axis=1)\n    if any(metric in METRICS_NEED_ORDER for metric in self.metrics):\n        kwargs[\"p\"] = p\n\n    emb_a = self.df.emb_a.to_numpy()\n    emb_b = self.df.emb_b.to_numpy()\n\n    emb_a_s = np.array_split(emb_a, np.ceil(experiment_size / batch_size))\n    emb_b_s = np.array_split(emb_b, np.ceil(experiment_size / batch_size))\n\n    for metric, metric_fn in zip(self.metrics, metric_fns):\n        self.df[metric] = np.hstack(\n            [metric_fn(X, i, j, **kwargs) for i, j in zip(emb_a_s, emb_b_s)],\n        )\n    if return_embeddings:\n        self.df[\"emb_a\"] = X[emb_a].tolist()\n        self.df[\"emb_b\"] = X[emb_b].tolist()\n\n    self.experiment_success = True\n    return self.df\n</code></pre>"},{"location":"api/#evalify.evalify.Experiment.tar_at_far","title":"<code>tar_at_far(far_values)</code>","text":"<p>Calculates TAR at specified FAR values for each metric.</p> <p>Parameters:</p> Name Type Description Default <code>far_values</code> <code>List[float]</code> <p>A list of False Accept Rates (FAR) to get TAR values for.</p> required <p>Returns:</p> Name Type Description <code>OrderedDict</code> <code>OrderedDict</code> <p>A dictionary with keys as metrics and values as dictionaries</p> <code>OrderedDict</code> <p>of FAR:TAR pairs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any FAR in far_values is not between 0 and 1.</p> Source code in <code>evalify/evalify.py</code> <pre><code>def tar_at_far(self, far_values: List[float]) -&gt; OrderedDict:\n    \"\"\"Calculates TAR at specified FAR values for each metric.\n\n    Args:\n        far_values (List[float]): A list of False Accept Rates (FAR) to get TAR\n            values for.\n\n    Returns:\n        OrderedDict: A dictionary with keys as metrics and values as dictionaries\n        of FAR:TAR pairs.\n\n    Raises:\n        ValueError: If any FAR in far_values is not between 0 and 1.\n    \"\"\"\n    if isinstance(far_values, (float, int)):\n        far_values = [float(far_values)]\n\n    if not all(0 &lt;= far &lt;= 1 for far in far_values):\n        raise ValueError(\"All FAR values must be between 0 and 1.\")\n\n    self.check_experiment_run()\n    tar_at_far_results = {}\n\n    for metric in self.metrics:\n        predicted = self.predicted_as_similarity(metric)\n        fpr, tpr, _ = roc_curve(self.df[\"target\"], predicted, pos_label=1)\n\n        tar_values = {}\n        for far in far_values:\n            idx = np.searchsorted(fpr, far, side=\"right\") - 1\n            idx = max(0, min(idx, len(fpr) - 1))  # Ensure idx is within bounds\n            tar_values[far] = tpr[idx].item()\n\n        tar_at_far_results[metric] = tar_values\n\n    self.tar_at_far_results = OrderedDict(\n        sorted(tar_at_far_results.items(), key=lambda x: list(x[1].keys())[0])\n    )\n\n    return self.tar_at_far_results\n</code></pre>"},{"location":"api/#evalify.evalify.Experiment.threshold_at_fpr","title":"<code>threshold_at_fpr(fpr)</code>","text":"<p>Find the threshold at a specified False Positive Rate (FPR) for each metric.</p> <p>The function calculates the threshold at the specified FPR for each metric by using the Receiver Operating Characteristic (ROC) curve. If the desired FPR is 0 or 1, or no exact match is found, the closest thresholds are used.</p> <p>Parameters:</p> Name Type Description Default <code>fpr</code> <code>float</code> <p>Desired False Positive Rate. Must be between 0 and 1.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary where keys are the metrics and values are dictionaries</p> <code>dict</code> <p>containing FPR, TPR, and threshold at the specified FPR.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided <code>fpr</code> is not between 0 and 1.</p> Source code in <code>evalify/evalify.py</code> <pre><code>def threshold_at_fpr(self, fpr: float) -&gt; dict:\n    \"\"\"Find the threshold at a specified False Positive Rate (FPR) for each metric.\n\n    The function calculates the threshold at the specified FPR for each metric\n    by using the Receiver Operating Characteristic (ROC) curve. If the desired\n    FPR is 0 or 1, or no exact match is found, the closest thresholds are used.\n\n    Args:\n        fpr (float): Desired False Positive Rate. Must be between 0 and 1.\n\n    Returns:\n        dict: A dictionary where keys are the metrics and values are dictionaries\n        containing FPR, TPR, and threshold at the specified FPR.\n\n    Raises:\n        ValueError: If the provided `fpr` is not between 0 and 1.\n    \"\"\"\n\n    self.check_experiment_run()\n    if not 0 &lt;= fpr &lt;= 1:\n        msg = \"`fpr` must be between 0 and 1. \" f\"Received wanted_fpr={fpr}\"\n        raise ValueError(\n            msg,\n        )\n    threshold_at_fpr = {}\n    for metric in self.metrics:\n        predicted = self.predicted_as_similarity(metric)\n        FPR, TPR, thresholds = roc_curve(\n            self.df[\"target\"],\n            predicted,\n            drop_intermediate=False,\n        )\n        df_fpr_tpr = pd.DataFrame({\"FPR\": FPR, \"TPR\": TPR, \"threshold\": thresholds})\n        ix_left = np.searchsorted(df_fpr_tpr[\"FPR\"], fpr, side=\"left\")\n        ix_right = np.searchsorted(df_fpr_tpr[\"FPR\"], fpr, side=\"right\")\n\n        if fpr == 0:\n            best = df_fpr_tpr.iloc[ix_right]\n        elif fpr == 1 or ix_left == ix_right:\n            best = df_fpr_tpr.iloc[ix_left]\n        else:\n            best = (\n                df_fpr_tpr.iloc[ix_left]\n                if abs(df_fpr_tpr.iloc[ix_left].FPR - fpr)\n                &lt; abs(df_fpr_tpr.iloc[ix_right].FPR - fpr)\n                else df_fpr_tpr.iloc[ix_right]\n            )\n        best = best.to_dict()\n        if metric in REVERSE_DISTANCE_TO_SIMILARITY:\n            best[\"threshold\"] = REVERSE_DISTANCE_TO_SIMILARITY.get(metric)(\n                best[\"threshold\"],\n            )\n        threshold_at_fpr[metric] = best\n    return threshold_at_fpr\n</code></pre>"},{"location":"authors/","title":"authors","text":""},{"location":"authors/#credits","title":"Credits","text":""},{"location":"authors/#development-lead","title":"Development Lead","text":"<ul> <li>Mahmoud Bahaa evalify@ma7555.anonaddy.com</li> </ul>"},{"location":"authors/#contributors","title":"Contributors","text":"<p>None yet. Why not be the first?</p>"},{"location":"authors/#others","title":"Others","text":"<ul> <li> <p>This package was created with Cookiecutter and the zillionare/cookiecutter-pypackage project template.</p> </li> <li> <p>Logo was created using font GlacialIndifference-Regular by Hanken Design Co.</p> </li> <li>Logo icon designed by Mauro Lucchesi</li> </ul>"},{"location":"contributing/","title":"contributing","text":""},{"location":"contributing/#contributing","title":"Contributing","text":"<p>Contributions are welcomed, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/ma7555/evalify/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>evalify could always use more documentation, whether as part of the official evalify docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/ma7555/evalify/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions   are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up <code>evalify</code> for local development.</p> <ol> <li>Fork the <code>evalify</code> repo on GitHub.</li> <li>Clone your fork locally</li> </ol> <pre><code>git clone git@github.com:your_name_here/evalify.git\n</code></pre> <ol> <li>Ensure poetry is installed.</li> <li>Install dependencies and start your virtualenv:</li> </ol> <pre><code>poetry install -E test -E doc -E dev\n</code></pre> <ol> <li>Create a branch for local development:</li> </ol> <pre><code>git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> <ol> <li>When you're done making changes, check that your changes pass the    tests, including testing other Python versions, with tox:</li> </ol> <pre><code>tox\n</code></pre> <ol> <li>Commit your changes and push your branch to GitHub:</li> </ol> <pre><code>git add .\ngit commit -m \"Your detailed description of your changes.\"\ngit push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put    your new functionality into a function with a docstring, and add the    feature to the list in README.md.</li> <li>The pull request should work for Python 3.9, 3.10, 3.11, 3.12 and for PyPy. Check    https://github.com/ma7555/evalify/actions    and make sure that the tests pass for all supported Python versions.</li> </ol>"},{"location":"contributing/#_1","title":"contributing","text":"<p><pre><code>python -m unittest\n</code></pre> or <pre><code>pytest\n</code></pre> To run a subset of tests.</p>"},{"location":"contributing/#deploying","title":"Deploying","text":"<p>A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in HISTORY.md). Then run:</p> <pre><code>git push\ngit push --tags\n</code></pre> <p>Github Actions will then deploy to PyPI if tests pass.</p>"},{"location":"history/","title":"history","text":""},{"location":"history/#history","title":"History","text":""},{"location":"history/#010-2022-02-20","title":"0.1.0 (2022-02-20)","text":"<ul> <li>First release on PyPI.</li> </ul>"},{"location":"history/#011-2022-02-22","title":"0.1.1 (2022-02-22)","text":"<ul> <li>Run time enhancement. </li> </ul>"},{"location":"history/#012-2022-02-23","title":"0.1.2 (2022-02-23)","text":"<ul> <li>Various enhancements and refactoring.</li> </ul>"},{"location":"history/#013-2022-02-24","title":"0.1.3 (2022-02-24)","text":"<ul> <li>Add pearson similarity as a metric</li> </ul>"},{"location":"history/#014-2022-02-24","title":"0.1.4 (2022-02-24)","text":"<ul> <li>Add EER calculation function.</li> <li>Drop support for python 3.7</li> </ul>"},{"location":"history/#100-2024-11-08","title":"1.0.0 (2024-11-08)","text":"<ul> <li>Bump dependencies.</li> <li>Drop support for python 3.8</li> <li>Add support for TAR @ FAR</li> </ul>"},{"location":"installation/","title":"installation","text":""},{"location":"installation/#installation","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install evalify, run this command in your terminal:</p> <pre><code>pip install evalify\n</code></pre> <p>This is the preferred method to install evalify, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-source","title":"From source","text":"<p>The source for evalify can be downloaded from the Github repo.</p> <p>You can either clone the public repository:</p> <pre><code>git clone git://github.com/ma7555/evalify\n</code></pre> <p>Or download the tarball:</p> <pre><code>curl -OJL https://github.com/ma7555/evalify/tarball/master\n</code></pre> <p>Once you have a copy of the source, you can install it with:</p> <pre><code>pip install .\n</code></pre>"},{"location":"usage/","title":"usage","text":""},{"location":"usage/#usage","title":"Usage","text":"<p>To use evalify in a project</p> <pre><code>import numpy as np\nfrom evalify import Experiment\n\nrng = np.random.default_rng()\nnphotos = 500\nemb_size = 32\nnclasses = 10\nX = rng.random((self.nphotos, self.emb_size))\ny = rng.integers(self.nclasses, size=self.nphotos)\n\nexperiment = Experiment()\nexperiment.run(X, y)\nexperiment.get_roc_auc()\nprint(experiment.df.roc_auc)\n</code></pre> <p>For a working experiment using real face embeddings, please refer to <code>LFW.py</code> under <code>./examples</code>.</p> <p><pre><code>python ./examples/LFW.py\n</code></pre> <pre><code>Total available embeddings 2921 resulted in 4264660 samples for the experiment.\nMetrics calculations executed in 24.05 seconds\nROC AUC:\nOrderedDict([('euclidean_distance', 0.9991302819624498), ('cosine_distance', 0.9991302818953706), ('euclidean_distance_l2', 0.9991302818953706), ('manhattan_distance', 0.9991260462584446)])\n</code></pre></p>"}]}