{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"evalify \u00b6 evalify contains tools needed to evaluate your face verification models literally in seconds. Free software: BSD-3-Clause Documentation: https://evalify.readthedocs.io Features \u00b6 TODO Contribution \u00b6 TODO Credits \u00b6 This package was created with Cookiecutter and the zillionare/cookiecutter-pypackage project template.","title":"home"},{"location":"#evalify","text":"evalify contains tools needed to evaluate your face verification models literally in seconds. Free software: BSD-3-Clause Documentation: https://evalify.readthedocs.io","title":"evalify"},{"location":"#features","text":"TODO","title":"Features"},{"location":"#contribution","text":"TODO","title":"Contribution"},{"location":"#credits","text":"This package was created with Cookiecutter and the zillionare/cookiecutter-pypackage project template.","title":"Credits"},{"location":"api/","text":"Top-level package for evalify. evalify \u00b6 Main module. create_experiment ( X , y , metric = 'cosine_similarity' , same_class_samples = 'full' , different_class_samples = 'minimal' , nsplits = 'best' , shuffle = False , return_embeddings = False ) \u00b6 Creates an experiment for face verification Parameters: Name Type Description Default X ndarray Embeddings array required y ndarray Targets for X as integers required metric str metric used for comparing embeddings distance 'cosine_similarity' same_class_samples Union[str, int] 'full': Sampling all possible pairs within each class int: Sampling specify number of pairs for every class. If the provided number is greater than the achievable for the class, the maximum possible combinations are used. 'full' different_class_samples str 'full': Samples all possible pairs within different classes. This can grow exponentially as the number of images increase. 'minimal': Samples one image from every class with one image of all other classes. (Default) int: Samples one image from every class with X images of every other class. If the provided number is greater than the achievable for the class, the maximum possible combinations are used. 'minimal' nsplits Union[str, int] 'best': Let the program decide based on available memory such that every split will fit into the available memory. (Default) int: Manually decide the number of splits. 'best' shuffle bool Whether to shuffle the returned experiment dataframe. Default: False. False return_embeddings bool Whether to return the embeddings instead of indexes. Default: False False Returns: Type Description pandas.DataFrame A DataFrame representing the experiment results. Exceptions: Type Description ValueError An error occurred with the provided arguments. Source code in evalify\\evalify.py def create_experiment ( X : np . ndarray , y : np . ndarray , metric : str = \"cosine_similarity\" , same_class_samples : Union [ str , int ] = \"full\" , different_class_samples : str = \"minimal\" , nsplits : Union [ str , int ] = \"best\" , shuffle : bool = False , return_embeddings : bool = False , ): \"\"\"Creates an experiment for face verification Args: X: Embeddings array y: Targets for X as integers metric: metric used for comparing embeddings distance same_class_samples: - 'full': Sampling all possible pairs within each class - int: Sampling specify number of pairs for every class. If the provided number is greater than the achievable for the class, the maximum possible combinations are used. different_class_samples: - 'full': Samples all possible pairs within different classes. This can grow exponentially as the number of images increase. - 'minimal': Samples one image from every class with one image of all other classes. (Default) - int: Samples one image from every class with X images of every other class. If the provided number is greater than the achievable for the class, the maximum possible combinations are used. nsplits: - 'best': Let the program decide based on available memory such that every split will fit into the available memory. (Default) - int: Manually decide the number of splits. shuffle: Whether to shuffle the returned experiment dataframe. Default: False. return_embeddings: Whether to return the embeddings instead of indexes. Default: False Returns: pandas.DataFrame: A DataFrame representing the experiment results. Raises: ValueError: An error occurred with the provided arguments. \"\"\" if same_class_samples != \"full\" and not isinstance ( same_class_samples , int ): raise ValueError ( '`same_class_samples` argument must be one of \"full\" or an integer ' f \"Received: same_class_samples= { same_class_samples } \" ) if different_class_samples not in ( \"full\" , \"minimal\" ) and not isinstance ( different_class_samples , int ): raise ValueError ( '`different_class_samples` argument must be one of \"full\", \"minimal\" ' \"or an integer \" f \"Received: different_class_samples= { different_class_samples } .\" ) if nsplits != \"best\" and not isinstance ( nsplits , int ): raise ValueError ( '`nsplits` argument must be either \"best\" or of type integer ' f \"Received: nsplits= { nsplits } with type { type ( nsplits ) } .\" ) if metric not in metrics_caller : raise ValueError ( f \"`metric` argument must be one of { tuple ( metrics_caller . keys ()) } \" f \"Received: metric= { metric } \" ) all_targets = np . unique ( y ) all_pairs = list () metric_fn = metrics_caller . get ( metric ) for target in all_targets : same_ixs = np . argwhere ( y == target ) . ravel () same_pairs = itertools . combinations ( same_ixs , 2 ) same_pairs = [( a , b , target , target , 1 ) for a , b in same_pairs ] different_ixs = np . argwhere ( y != target ) . ravel () df = pd . DataFrame ( data = { \"ix\" : different_ixs , \"target\" : y [ different_ixs ]}) if different_class_samples == \"minimal\" : df = df . sample ( frac = 1 ) . drop_duplicates ( subset = [ \"target\" ]) different_ixs = df . ix . to_numpy () different_pairs = itertools . product ([ np . random . choice ( same_ixs )], different_ixs ) different_pairs = [( a , b , target , y [ b ], 1 ) for a , b in different_pairs ] all_pairs += same_pairs + different_pairs one_to_one_df = pd . DataFrame ( data = all_pairs , columns = [ \"img_a\" , \"img_b\" , \"target_a\" , \"target_b\" , \"target\" ] ) if shuffle : one_to_one_df = one_to_one_df . frac ( 1 ) if nsplits == \"best\" : nsplits = calculate_best_split_size ( X , len ( one_to_one_df )) Xs = np . array_split ( one_to_one_df . img_a . to_numpy (), nsplits ) ys = np . array_split ( one_to_one_df . img_b . to_numpy (), nsplits ) if metric in [ \"cosine_similarity\" , \"euclidean_distance_l2\" ]: norms = np . linalg . norm ( X , axis = 1 ) else : norms = None one_to_one_df [ metric ] = np . hstack ( [ metric_fn ( X , ix , iy , norms ) for ( ix , iy ) in zip ( Xs , ys )] ) if return_embeddings : one_to_one_df [ \"img_a\" ] = X [ one_to_one_df . img_a . to_numpy ()] . tolist () one_to_one_df [ \"img_b\" ] = X [ one_to_one_df . img_b . to_numpy ()] . tolist () return one_to_one_df metrics \u00b6 Metrics module. utils \u00b6 calculate_best_split_size ( X , experiment_size ) \u00b6 Calculate best number of splits. Source code in evalify\\utils.py def calculate_best_split_size ( X , experiment_size ): \"\"\"Calculate best number of splits.\"\"\" available_mem = _calc_available_memory () max_rows = _keep_to_max_rows ( X , available_mem ) nsplits = int ( experiment_size / max_rows ) # add a split to avoid 0 splits nsplits += 1 return nsplits find_optimal_cutoff ( target , predicted ) \u00b6 Find the optimal cutoff point Parameters: Name Type Description Default target Matrix with dependent or target data, where rows are observations required predicted Matrix with predicted data, where rows are observations required Returns: Type Description float type, with optimal cutoff value Source code in evalify\\utils.py def find_optimal_cutoff ( target , predicted ): \"\"\"Find the optimal cutoff point Args: target: Matrix with dependent or target data, where rows are observations predicted: Matrix with predicted data, where rows are observations Returns: float type, with optimal cutoff value \"\"\" fpr , tpr , threshold = roc_curve ( target , predicted ) i = np . arange ( len ( tpr )) roc = pd . DataFrame ( { \"tf\" : pd . Series ( tpr - ( 1 - fpr ), index = i ), \"threshold\" : pd . Series ( threshold , index = i ), } ) roc_t = roc . iloc [( roc . tf - 0 ) . abs () . argsort ()[: 1 ]] return roc_t [ \"threshold\" ] . item ()","title":"modules"},{"location":"api/#evalify.evalify","text":"Main module.","title":"evalify"},{"location":"api/#evalify.evalify.create_experiment","text":"Creates an experiment for face verification Parameters: Name Type Description Default X ndarray Embeddings array required y ndarray Targets for X as integers required metric str metric used for comparing embeddings distance 'cosine_similarity' same_class_samples Union[str, int] 'full': Sampling all possible pairs within each class int: Sampling specify number of pairs for every class. If the provided number is greater than the achievable for the class, the maximum possible combinations are used. 'full' different_class_samples str 'full': Samples all possible pairs within different classes. This can grow exponentially as the number of images increase. 'minimal': Samples one image from every class with one image of all other classes. (Default) int: Samples one image from every class with X images of every other class. If the provided number is greater than the achievable for the class, the maximum possible combinations are used. 'minimal' nsplits Union[str, int] 'best': Let the program decide based on available memory such that every split will fit into the available memory. (Default) int: Manually decide the number of splits. 'best' shuffle bool Whether to shuffle the returned experiment dataframe. Default: False. False return_embeddings bool Whether to return the embeddings instead of indexes. Default: False False Returns: Type Description pandas.DataFrame A DataFrame representing the experiment results. Exceptions: Type Description ValueError An error occurred with the provided arguments. Source code in evalify\\evalify.py def create_experiment ( X : np . ndarray , y : np . ndarray , metric : str = \"cosine_similarity\" , same_class_samples : Union [ str , int ] = \"full\" , different_class_samples : str = \"minimal\" , nsplits : Union [ str , int ] = \"best\" , shuffle : bool = False , return_embeddings : bool = False , ): \"\"\"Creates an experiment for face verification Args: X: Embeddings array y: Targets for X as integers metric: metric used for comparing embeddings distance same_class_samples: - 'full': Sampling all possible pairs within each class - int: Sampling specify number of pairs for every class. If the provided number is greater than the achievable for the class, the maximum possible combinations are used. different_class_samples: - 'full': Samples all possible pairs within different classes. This can grow exponentially as the number of images increase. - 'minimal': Samples one image from every class with one image of all other classes. (Default) - int: Samples one image from every class with X images of every other class. If the provided number is greater than the achievable for the class, the maximum possible combinations are used. nsplits: - 'best': Let the program decide based on available memory such that every split will fit into the available memory. (Default) - int: Manually decide the number of splits. shuffle: Whether to shuffle the returned experiment dataframe. Default: False. return_embeddings: Whether to return the embeddings instead of indexes. Default: False Returns: pandas.DataFrame: A DataFrame representing the experiment results. Raises: ValueError: An error occurred with the provided arguments. \"\"\" if same_class_samples != \"full\" and not isinstance ( same_class_samples , int ): raise ValueError ( '`same_class_samples` argument must be one of \"full\" or an integer ' f \"Received: same_class_samples= { same_class_samples } \" ) if different_class_samples not in ( \"full\" , \"minimal\" ) and not isinstance ( different_class_samples , int ): raise ValueError ( '`different_class_samples` argument must be one of \"full\", \"minimal\" ' \"or an integer \" f \"Received: different_class_samples= { different_class_samples } .\" ) if nsplits != \"best\" and not isinstance ( nsplits , int ): raise ValueError ( '`nsplits` argument must be either \"best\" or of type integer ' f \"Received: nsplits= { nsplits } with type { type ( nsplits ) } .\" ) if metric not in metrics_caller : raise ValueError ( f \"`metric` argument must be one of { tuple ( metrics_caller . keys ()) } \" f \"Received: metric= { metric } \" ) all_targets = np . unique ( y ) all_pairs = list () metric_fn = metrics_caller . get ( metric ) for target in all_targets : same_ixs = np . argwhere ( y == target ) . ravel () same_pairs = itertools . combinations ( same_ixs , 2 ) same_pairs = [( a , b , target , target , 1 ) for a , b in same_pairs ] different_ixs = np . argwhere ( y != target ) . ravel () df = pd . DataFrame ( data = { \"ix\" : different_ixs , \"target\" : y [ different_ixs ]}) if different_class_samples == \"minimal\" : df = df . sample ( frac = 1 ) . drop_duplicates ( subset = [ \"target\" ]) different_ixs = df . ix . to_numpy () different_pairs = itertools . product ([ np . random . choice ( same_ixs )], different_ixs ) different_pairs = [( a , b , target , y [ b ], 1 ) for a , b in different_pairs ] all_pairs += same_pairs + different_pairs one_to_one_df = pd . DataFrame ( data = all_pairs , columns = [ \"img_a\" , \"img_b\" , \"target_a\" , \"target_b\" , \"target\" ] ) if shuffle : one_to_one_df = one_to_one_df . frac ( 1 ) if nsplits == \"best\" : nsplits = calculate_best_split_size ( X , len ( one_to_one_df )) Xs = np . array_split ( one_to_one_df . img_a . to_numpy (), nsplits ) ys = np . array_split ( one_to_one_df . img_b . to_numpy (), nsplits ) if metric in [ \"cosine_similarity\" , \"euclidean_distance_l2\" ]: norms = np . linalg . norm ( X , axis = 1 ) else : norms = None one_to_one_df [ metric ] = np . hstack ( [ metric_fn ( X , ix , iy , norms ) for ( ix , iy ) in zip ( Xs , ys )] ) if return_embeddings : one_to_one_df [ \"img_a\" ] = X [ one_to_one_df . img_a . to_numpy ()] . tolist () one_to_one_df [ \"img_b\" ] = X [ one_to_one_df . img_b . to_numpy ()] . tolist () return one_to_one_df","title":"create_experiment()"},{"location":"api/#evalify.metrics","text":"Metrics module.","title":"metrics"},{"location":"api/#evalify.utils","text":"","title":"utils"},{"location":"api/#evalify.utils.calculate_best_split_size","text":"Calculate best number of splits. Source code in evalify\\utils.py def calculate_best_split_size ( X , experiment_size ): \"\"\"Calculate best number of splits.\"\"\" available_mem = _calc_available_memory () max_rows = _keep_to_max_rows ( X , available_mem ) nsplits = int ( experiment_size / max_rows ) # add a split to avoid 0 splits nsplits += 1 return nsplits","title":"calculate_best_split_size()"},{"location":"api/#evalify.utils.find_optimal_cutoff","text":"Find the optimal cutoff point Parameters: Name Type Description Default target Matrix with dependent or target data, where rows are observations required predicted Matrix with predicted data, where rows are observations required Returns: Type Description float type, with optimal cutoff value Source code in evalify\\utils.py def find_optimal_cutoff ( target , predicted ): \"\"\"Find the optimal cutoff point Args: target: Matrix with dependent or target data, where rows are observations predicted: Matrix with predicted data, where rows are observations Returns: float type, with optimal cutoff value \"\"\" fpr , tpr , threshold = roc_curve ( target , predicted ) i = np . arange ( len ( tpr )) roc = pd . DataFrame ( { \"tf\" : pd . Series ( tpr - ( 1 - fpr ), index = i ), \"threshold\" : pd . Series ( threshold , index = i ), } ) roc_t = roc . iloc [( roc . tf - 0 ) . abs () . argsort ()[: 1 ]] return roc_t [ \"threshold\" ] . item ()","title":"find_optimal_cutoff()"},{"location":"authors/","text":"Credits \u00b6 Development Lead \u00b6 Mahmoud Bahaa mah.alaa@nu.edu.eg Contributors \u00b6 None yet. Why not be the first?","title":"authors"},{"location":"authors/#credits","text":"","title":"Credits"},{"location":"authors/#development-lead","text":"Mahmoud Bahaa mah.alaa@nu.edu.eg","title":"Development Lead"},{"location":"authors/#contributors","text":"None yet. Why not be the first?","title":"Contributors"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/ma7555/evalify/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation \u00b6 evalify could always use more documentation, whether as part of the official evalify docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/ma7555/evalify/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up evalify for local development. Fork the evalify repo on GitHub. Clone your fork locally 1 $ git clone git@github.com:your_name_here/evalify.git Ensure poetry is installed. Install dependencies and start your virtualenv: 1 $ poetry install -E test -E doc -E dev Create a branch for local development: 1 $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: 1 $ tox Commit your changes and push your branch to GitHub: 1 2 3 $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.6, 3.7, 3.8, 3.9 and for PyPy. Check https://github.com/ma7555/evalify/actions and make sure that the tests pass for all supported Python versions. Tips``` \u00b6 1 $ python -m unittest tests.test_evalify ```To run a subset of tests. Deploying \u00b6 A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in HISTORY.md). Then run: 1 2 3 $ poetry patch # possible: major / minor / patch $ git push $ git push --tags Travis will then deploy to PyPI if tests pass.","title":"contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/ma7555/evalify/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"evalify could always use more documentation, whether as part of the official evalify docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/ma7555/evalify/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up evalify for local development. Fork the evalify repo on GitHub. Clone your fork locally 1 $ git clone git@github.com:your_name_here/evalify.git Ensure poetry is installed. Install dependencies and start your virtualenv: 1 $ poetry install -E test -E doc -E dev Create a branch for local development: 1 $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: 1 $ tox Commit your changes and push your branch to GitHub: 1 2 3 $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.6, 3.7, 3.8, 3.9 and for PyPy. Check https://github.com/ma7555/evalify/actions and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"contributing/#tips","text":"1 $ python -m unittest tests.test_evalify ```To run a subset of tests.","title":"Tips```"},{"location":"contributing/#deploying","text":"A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in HISTORY.md). Then run: 1 2 3 $ poetry patch # possible: major / minor / patch $ git push $ git push --tags Travis will then deploy to PyPI if tests pass.","title":"Deploying"},{"location":"history/","text":"History \u00b6 0.1.0 (2022-02-15) \u00b6 First release on PyPI.","title":"history"},{"location":"history/#history","text":"","title":"History"},{"location":"history/#010-2022-02-15","text":"First release on PyPI.","title":"0.1.0 (2022-02-15)"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install evalify, run this command in your terminal: 1 $ pip install evalify This is the preferred method to install evalify, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From source \u00b6 The source for evalify can be downloaded from the Github repo . You can either clone the public repository: 1 $ git clone git://github.com/ma7555/evalify Or download the tarball : 1 $ curl -OJL https://github.com/ma7555/evalify/tarball/master Once you have a copy of the source, you can install it with: 1 $ pip install .","title":"installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install evalify, run this command in your terminal: 1 $ pip install evalify This is the preferred method to install evalify, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-source","text":"The source for evalify can be downloaded from the Github repo . You can either clone the public repository: 1 $ git clone git://github.com/ma7555/evalify Or download the tarball : 1 $ curl -OJL https://github.com/ma7555/evalify/tarball/master Once you have a copy of the source, you can install it with: 1 $ pip install .","title":"From source"},{"location":"usage/","text":"Usage \u00b6 To use evalify in a project 1 import evalify","title":"usage"},{"location":"usage/#usage","text":"To use evalify in a project 1 import evalify","title":"Usage"}]}