{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"evalify \u00b6 Evaluate your face or voice verification models literally in seconds. Installation \u00b6 Stable release \u00b6 1 pip install evalify Bleeding edge \u00b6 From source 1 pip install git+https://github.com/ma7555/evalify.git From TestPyPI 1 pip install --index-url https://test.pypi.org/simple/ evalify Usage \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import numpy as np from evalify import Experiment rng = np . random . default_rng () nphotos = 500 emb_size = 32 nclasses = 10 X = rng . random (( self . nphotos , self . emb_size )) y = rng . integers ( self . nclasses , size = self . nphotos ) experiment = Experiment () experiment . run ( X , y ) experiment . get_roc_auc () print ( experiment . df . roc_auc ) Documentation: \u00b6 https://ma7555.github.io/evalify/ Features \u00b6 Blazing fast implementation for metrics calculation through optimized einstein sum and vectorized calculations. Many operations are dispatched to canonical BLAS, cuBLAS, or other specialized routines. Smart sampling options using direct indexing from pre-calculated arrays with an option to have total control over sampling strategy and sampling numbers. Supports most evaluation metrics: cosine_similarity cosine_distance euclidean_distance euclidean_distance_l2 minkowski_distance manhattan_distance chebyshev_distance Computation time for 4 metrics 4.2 million samples experiment is 24 seconds vs 51 minutes if looping using scipy.spatial.distance implemntations. TODO \u00b6 Safer memory allocation. I did not have issues but if you ran out of memory please manually increase number of splits with nsplits argument. Contribution \u00b6 Contributions are welcomed, and they are greatly appreciated! Every little bit helps, and credit will always be given. Please check CONTRIBUTING.md for guidelines. Citation \u00b6 If you use this software, please cite it using the metadata from CITATION.cff","title":"home"},{"location":"#evalify","text":"Evaluate your face or voice verification models literally in seconds.","title":"evalify"},{"location":"#installation","text":"","title":"Installation"},{"location":"#stable-release","text":"1 pip install evalify","title":"Stable release"},{"location":"#bleeding-edge","text":"From source 1 pip install git+https://github.com/ma7555/evalify.git From TestPyPI 1 pip install --index-url https://test.pypi.org/simple/ evalify","title":"Bleeding edge"},{"location":"#usage","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 import numpy as np from evalify import Experiment rng = np . random . default_rng () nphotos = 500 emb_size = 32 nclasses = 10 X = rng . random (( self . nphotos , self . emb_size )) y = rng . integers ( self . nclasses , size = self . nphotos ) experiment = Experiment () experiment . run ( X , y ) experiment . get_roc_auc () print ( experiment . df . roc_auc )","title":"Usage"},{"location":"#documentation","text":"https://ma7555.github.io/evalify/","title":"Documentation:"},{"location":"#features","text":"Blazing fast implementation for metrics calculation through optimized einstein sum and vectorized calculations. Many operations are dispatched to canonical BLAS, cuBLAS, or other specialized routines. Smart sampling options using direct indexing from pre-calculated arrays with an option to have total control over sampling strategy and sampling numbers. Supports most evaluation metrics: cosine_similarity cosine_distance euclidean_distance euclidean_distance_l2 minkowski_distance manhattan_distance chebyshev_distance Computation time for 4 metrics 4.2 million samples experiment is 24 seconds vs 51 minutes if looping using scipy.spatial.distance implemntations.","title":"Features"},{"location":"#todo","text":"Safer memory allocation. I did not have issues but if you ran out of memory please manually increase number of splits with nsplits argument.","title":"TODO"},{"location":"#contribution","text":"Contributions are welcomed, and they are greatly appreciated! Every little bit helps, and credit will always be given. Please check CONTRIBUTING.md for guidelines.","title":"Contribution"},{"location":"#citation","text":"If you use this software, please cite it using the metadata from CITATION.cff","title":"Citation"},{"location":"api/","text":"Top-level package for evalify. evalify \u00b6 Evalify main module used for creating the verification experiments. Creates experiments with embedding pairs to compare for face verification tasks including positive pairs, negative pairs and metrics calculations using a very optimized einstein sum. Many operations are dispatched to canonical BLAS, cuBLAS, or other specialized routines. Extremely large arrays are split into smaller batches, every batch would consume the roughly the maximum available memory. Typical usage example: 1 2 experiment = Experiment() experiment.run(X, y) Experiment \u00b6 Source code in evalify/evalify.py class Experiment : def __init__ ( self ) -> None : self . experiment_sucess = False self . cached_predicted_as_similarity = {} def __call__ ( self , * args : Any , ** kwds : Any ) -> Any : return self . run ( * args , ** kwds ) def run ( self , X : np . ndarray , y : np . ndarray , metrics : Union [ str , Sequence [ str ]] = \"cosine_similarity\" , same_class_samples : T_str_int = \"full\" , different_class_samples : Union [ str , int , Sequence [ T_str_int ]] = \"minimal\" , batch_size : T_str_int = \"best\" , shuffle : bool = False , seed : int = None , return_embeddings : bool = False , p : int = 3 , ): \"\"\"Runs an experiment for face verification Args: X: Embeddings array y: Targets for X as integers metrics: - 'cosine_similarity' - 'cosine_distance' - 'euclidean_distance' - 'euclidean_distance_l2' - 'minkowski_distance' - 'manhattan_distance' - 'chebyshev_distance' - list/tuple containing more than one of them. same_class_samples: - 'full': Samples all possible images within each class to create all all possible positive pairs. - int: Samples specific number of images for every class to create nC2 pairs where n is passed integer. different_class_samples: - 'full': Samples one image from every class with all possible pairs of different classes. This can grow exponentially as the number of images increase. (N, M) = (1, \"full\") - 'minimal': Samples one image from every class with one image of all other classes. (N, M) = (1, 1). (Default) - int: Samples one image from every class with provided number of images of every other class. - tuple or list: (N, M) Samples N images from every class with M images of every other class. batch_size: - 'best': Let the program decide based on available memory such that every batch will fit into the available memory. (Default) - int: Manually decide the batch_size. shuffle: Whether to shuffle the returned experiment dataframe. Default: False. return_embeddings: Whether to return the embeddings instead of indexes. Default: False p: The order of the norm of the difference. Should be `0 < p < 1`, Only valid with minkowski_distance as a metric. Default = 3 Returns: pandas.DataFrame: A DataFrame representing the experiment results. Raises: ValueError: An error occurred with the provided arguments. Notes: same_class_samples: If the provided number is greater than the achievable for the class, the maximum possible combinations are used. different_class_samples: If the provided number is greater than the achievable for the class, the maximum possible combinations are used. (N, M) can also be ('full', 'full') but this will calculate all possible combinations between all posibile negative samples. If the dataset is not small this will probably result in an extremely large array!. \"\"\" if isinstance ( metrics , str ): metrics = ( metrics ,) self . _validate_args ( metrics , same_class_samples , different_class_samples , batch_size , p ) X , y = _validate_vectors ( X , y ) all_targets = np . unique ( y ) all_pairs = [] metric_fns = list ( map ( metrics_caller . get , metrics )) self . seed = seed self . rng = np . random . default_rng ( self . seed ) for target in all_targets : all_pairs += self . get_pairs ( y , same_class_samples , different_class_samples , target , ) self . df = pd . DataFrame ( data = all_pairs , columns = [ \"img_a\" , \"img_b\" , \"target_a\" , \"target_b\" , \"target\" ] ) if shuffle : self . df = self . df . sample ( frac = 1 , random_state = seed ) if batch_size == \"best\" : batch_size = calculate_best_batch_size ( X ) kwargs = {} if any ( metric in METRICS_NEED_NORM for metric in metrics ): kwargs [ \"norms\" ] = get_norms ( X ) if any ( metric in METRICS_NEED_ORDER for metric in metrics ): kwargs [ \"p\" ] = p img_a = self . df . img_a . to_numpy () img_b = self . df . img_b . to_numpy () experiment_size = len ( self . df ) img_a_s = np . array_split ( img_a , np . ceil ( experiment_size / batch_size )) img_b_s = np . array_split ( img_b , np . ceil ( experiment_size / batch_size )) for metric , metric_fn in zip ( metrics , metric_fns ): self . df [ metric ] = np . hstack ( [ metric_fn ( X , i , j , ** kwargs ) for i , j in zip ( img_a_s , img_b_s )] ) if return_embeddings : self . df [ \"img_a\" ] = X [ img_a ] . tolist () self . df [ \"img_b\" ] = X [ img_b ] . tolist () self . experiment_sucess = True self . metrics = metrics return self . df def get_pairs ( self , y , same_class_samples , different_class_samples , target , ): same_ixs_full = np . argwhere ( y == target ) . ravel () if isinstance ( same_class_samples , int ): same_class_samples = min ( len ( same_ixs_full ), same_class_samples ) same_ixs = self . rng . choice ( same_ixs_full , same_class_samples ) elif same_class_samples == \"full\" : same_ixs = same_ixs_full same_pairs = itertools . combinations ( same_ixs , 2 ) same_pairs = [( a , b , target , target , 1 ) for a , b in same_pairs ] different_ixs = np . argwhere ( y != target ) . ravel () diff_df = pd . DataFrame ( data = { \"ix\" : different_ixs , \"target\" : y [ different_ixs ]}) diff_df = diff_df . sample ( frac = 1 , random_state = self . seed ) if different_class_samples in [ \"full\" , \"minimal\" ] or isinstance ( different_class_samples , int ): N = 1 if different_class_samples == \"minimal\" : diff_df = diff_df . drop_duplicates ( subset = [ \"target\" ]) else : N , M = different_class_samples N = len ( same_ixs_full ) if N == \"full\" else min ( N , len ( same_ixs_full )) if M != \"full\" : diff_df = diff_df . groupby ( \"target\" ) . apply ( lambda x : x [: M ]) . droplevel ( 0 ) different_ixs = diff_df . ix . to_numpy () different_pairs = itertools . product ( self . rng . choice ( same_ixs_full , N , replace = False ), different_ixs ) different_pairs = [( a , b , target , y [ b ], 0 ) for a , b in different_pairs if a < b ] return same_pairs + different_pairs def _validate_args ( self , metrics , same_class_samples , different_class_samples , batch_size , p ): if same_class_samples != \"full\" and not isinstance ( same_class_samples , int ): raise ValueError ( \"`same_class_samples` argument must be one of 'full' or an integer \" f \"Received: same_class_samples= { same_class_samples } \" ) if different_class_samples not in ( \"full\" , \"minimal\" ): if not isinstance ( different_class_samples , ( int , list , tuple )): raise ValueError ( \"`different_class_samples` argument must be one of 'full', 'minimal', \" \"an integer, a list or tuple of integers or keyword 'full'.\" f \"Received: different_class_samples= { different_class_samples } .\" ) elif isinstance ( different_class_samples , ( list , tuple )): if ( not ( all ( isinstance ( i , int ) or i == \"full\" for i in different_class_samples ) ) or ( len ( different_class_samples )) != 2 ): raise ValueError ( \"When passing `different_class_samples` as a tuple or list, \" \"elements must be exactly two of integer type or keyword 'full' \" \"(N, M). \" f \"Received: different_class_samples= { different_class_samples } .\" ) if batch_size != \"best\" and not isinstance ( batch_size , int ): raise ValueError ( '`batch_size` argument must be either \"best\" or of type integer ' f \"Received: batch_size= { batch_size } with type { type ( batch_size ) } .\" ) if any ( metric not in metrics_caller for metric in metrics ): raise ValueError ( f \"`metric` argument must be one of { tuple ( metrics_caller . keys ()) } \" f \"Received: metric= { metrics } \" ) if p < 1 : raise ValueError ( f \"`p` must be at least 1. Received: p= { p } \" ) def find_optimal_cutoff ( self ): \"\"\"Find the optimal cutoff point Returns: float: optimal cutoff value \"\"\" self . check_experiment_run () self . optimal_cutoff = {} for metric in self . metrics : fpr , tpr , threshold = roc_curve ( self . df [ \"target\" ], self . df [ metric ]) i = np . arange ( len ( tpr )) roc = pd . DataFrame ( { \"tf\" : pd . Series ( tpr - ( 1 - fpr ), index = i ), \"threshold\" : pd . Series ( threshold , index = i ), } ) roc_t = roc . iloc [( roc . tf - 0 ) . abs () . argsort ()[: 1 ]] self . optimal_cutoff [ metric ] = roc_t [ \"threshold\" ] . item () return self . optimal_cutoff def find_threshold_at_fpr ( self , fpr : float ): \"\"\"Runs an experiment for face verification Args: fpr: False positive rate to find best threshold for. Returns: dict: A dictionary with keys as metrics and values as thresholds. Raises: ValueError: If `fpr` is not between 0 and 1. \"\"\" self . check_experiment_run () if not 0 <= fpr <= 1 : raise ValueError ( \"`fpr` must be between 0 and 1. \" f \"Received wanted_fpr= { fpr } \" ) threshold_at_fpr = {} for metric in self . metrics : predicted = self . predicted_as_similarity ( metric ) FPR , TPR , thresholds = roc_curve ( self . df [ \"target\" ], predicted , drop_intermediate = False ) df_fpr_tpr = pd . DataFrame ({ \"FPR\" : FPR , \"TPR\" : TPR , \"Threshold\" : thresholds }) ix_left = np . searchsorted ( df_fpr_tpr [ \"FPR\" ], fpr , side = \"left\" ) ix_right = np . searchsorted ( df_fpr_tpr [ \"FPR\" ], fpr , side = \"right\" ) if fpr == 0 : best = df_fpr_tpr . iloc [ ix_right ] elif fpr == 1 or ix_left == ix_right : best = df_fpr_tpr . iloc [ ix_left ] else : best = ( df_fpr_tpr . iloc [ ix_left ] if abs ( df_fpr_tpr . iloc [ ix_left ] . FPR - fpr ) < abs ( df_fpr_tpr . iloc [ ix_right ] . FPR - fpr ) else df_fpr_tpr . iloc [ ix_right ] ) best = best . to_dict () if metric in REVERSE_DISTANCE_TO_SIMILARITY : best [ \"Threshold\" ] = REVERSE_DISTANCE_TO_SIMILARITY . get ( metric )( best [ \"Threshold\" ] ) threshold_at_fpr [ metric ] = best return threshold_at_fpr def get_binary_prediction ( self , metric , threshold ): return ( self . df [ metric ] . apply ( lambda x : 1 if x < threshold else 0 ) if metric in DISTANCE_TO_SIMILARITY else self . df [ metric ] . apply ( lambda x : 1 if x > threshold else 0 ) ) def evaluate_at_threshold ( self , threshold : float , metric : str ): \"\"\"Evaluate performance at specific threshold Args: threshold: cut-off threshold. metric: metric to use. Returns: dict: containing all evaluation metrics. \"\"\" self . metrics_evaluation = {} self . check_experiment_run ( metric ) for metric in self . metrics : predicted = self . get_binary_prediction ( metric , threshold ) cm = confusion_matrix ( self . df [ \"target\" ], predicted ) tn , fp , fn , tp = cm . ravel () TPR = tp / ( tp + fn ) # recall / true positive rate TNR = tn / ( tn + fp ) # true negative rate PPV = tp / ( tp + fp ) # precision / positive predicted value NPV = tn / ( tn + fn ) # negative predictive value FPR = fp / ( fp + tn ) # false positive rate FNR = 1 - TPR # false negative rate FDR = 1 - PPV # false discovery rate FOR = 1 - NPV # false omission rate F1 = 2 * ( PPV * TPR ) / ( PPV + TPR ) # LRp = TPR / FPR # positive likelihood ratio (LR+) # LRn = FNR / TNR # negative likelihood ratio (LR+) evaluation = { \"TPR\" : TPR , \"TNR\" : TNR , \"PPV\" : PPV , \"NPV\" : NPV , \"FPR\" : FPR , \"FNR\" : FNR , \"FDR\" : FDR , \"FOR\" : FOR , \"F1\" : F1 , # \"LR+\": LRp, # \"LR-\": LRn, } # self.metrics_evaluation[metric] = evaluation return evaluation def check_experiment_run ( self , metric = None ): caller = sys . _getframe () . f_back . f_code . co_name if not self . experiment_sucess : raise NotImplementedError ( f \"` { caller } ` function can only be run after running \" \"`run_experiment`.\" ) if metric is not None and metric not in self . metrics : raise ValueError ( f \"` { caller } ` function was can only be called with `metric` from \" f \" { self . metrics } which were used while running the experiment\" ) def get_roc_auc ( self ): self . check_experiment_run () self . roc_auc = {} for metric in self . metrics : predicted = self . predicted_as_similarity ( metric ) fpr , tpr , thresholds = roc_curve ( self . df [ \"target\" ], predicted , drop_intermediate = False ) self . roc_auc [ metric ] = auc ( fpr , tpr ) self . roc_auc = OrderedDict ( sorted ( self . roc_auc . items (), key = lambda x : x [ 1 ], reverse = True ) ) return self . roc_auc def predicted_as_similarity ( self , metric ): predicted = self . df [ metric ] if metric in DISTANCE_TO_SIMILARITY : predicted = ( self . cached_predicted_as_similarity [ metric ] if metric in self . cached_predicted_as_similarity else DISTANCE_TO_SIMILARITY . get ( metric )( predicted ) ) return predicted evaluate_at_threshold ( self , threshold , metric ) \u00b6 Evaluate performance at specific threshold Parameters: Name Type Description Default threshold float cut-off threshold. required metric str metric to use. required Returns: Type Description dict containing all evaluation metrics. Source code in evalify/evalify.py def evaluate_at_threshold ( self , threshold : float , metric : str ): \"\"\"Evaluate performance at specific threshold Args: threshold: cut-off threshold. metric: metric to use. Returns: dict: containing all evaluation metrics. \"\"\" self . metrics_evaluation = {} self . check_experiment_run ( metric ) for metric in self . metrics : predicted = self . get_binary_prediction ( metric , threshold ) cm = confusion_matrix ( self . df [ \"target\" ], predicted ) tn , fp , fn , tp = cm . ravel () TPR = tp / ( tp + fn ) # recall / true positive rate TNR = tn / ( tn + fp ) # true negative rate PPV = tp / ( tp + fp ) # precision / positive predicted value NPV = tn / ( tn + fn ) # negative predictive value FPR = fp / ( fp + tn ) # false positive rate FNR = 1 - TPR # false negative rate FDR = 1 - PPV # false discovery rate FOR = 1 - NPV # false omission rate F1 = 2 * ( PPV * TPR ) / ( PPV + TPR ) # LRp = TPR / FPR # positive likelihood ratio (LR+) # LRn = FNR / TNR # negative likelihood ratio (LR+) evaluation = { \"TPR\" : TPR , \"TNR\" : TNR , \"PPV\" : PPV , \"NPV\" : NPV , \"FPR\" : FPR , \"FNR\" : FNR , \"FDR\" : FDR , \"FOR\" : FOR , \"F1\" : F1 , # \"LR+\": LRp, # \"LR-\": LRn, } # self.metrics_evaluation[metric] = evaluation return evaluation find_optimal_cutoff ( self ) \u00b6 Find the optimal cutoff point Returns: Type Description float optimal cutoff value Source code in evalify/evalify.py def find_optimal_cutoff ( self ): \"\"\"Find the optimal cutoff point Returns: float: optimal cutoff value \"\"\" self . check_experiment_run () self . optimal_cutoff = {} for metric in self . metrics : fpr , tpr , threshold = roc_curve ( self . df [ \"target\" ], self . df [ metric ]) i = np . arange ( len ( tpr )) roc = pd . DataFrame ( { \"tf\" : pd . Series ( tpr - ( 1 - fpr ), index = i ), \"threshold\" : pd . Series ( threshold , index = i ), } ) roc_t = roc . iloc [( roc . tf - 0 ) . abs () . argsort ()[: 1 ]] self . optimal_cutoff [ metric ] = roc_t [ \"threshold\" ] . item () return self . optimal_cutoff find_threshold_at_fpr ( self , fpr ) \u00b6 Runs an experiment for face verification Parameters: Name Type Description Default fpr float False positive rate to find best threshold for. required Returns: Type Description dict A dictionary with keys as metrics and values as thresholds. Exceptions: Type Description ValueError If fpr is not between 0 and 1. Source code in evalify/evalify.py def find_threshold_at_fpr ( self , fpr : float ): \"\"\"Runs an experiment for face verification Args: fpr: False positive rate to find best threshold for. Returns: dict: A dictionary with keys as metrics and values as thresholds. Raises: ValueError: If `fpr` is not between 0 and 1. \"\"\" self . check_experiment_run () if not 0 <= fpr <= 1 : raise ValueError ( \"`fpr` must be between 0 and 1. \" f \"Received wanted_fpr= { fpr } \" ) threshold_at_fpr = {} for metric in self . metrics : predicted = self . predicted_as_similarity ( metric ) FPR , TPR , thresholds = roc_curve ( self . df [ \"target\" ], predicted , drop_intermediate = False ) df_fpr_tpr = pd . DataFrame ({ \"FPR\" : FPR , \"TPR\" : TPR , \"Threshold\" : thresholds }) ix_left = np . searchsorted ( df_fpr_tpr [ \"FPR\" ], fpr , side = \"left\" ) ix_right = np . searchsorted ( df_fpr_tpr [ \"FPR\" ], fpr , side = \"right\" ) if fpr == 0 : best = df_fpr_tpr . iloc [ ix_right ] elif fpr == 1 or ix_left == ix_right : best = df_fpr_tpr . iloc [ ix_left ] else : best = ( df_fpr_tpr . iloc [ ix_left ] if abs ( df_fpr_tpr . iloc [ ix_left ] . FPR - fpr ) < abs ( df_fpr_tpr . iloc [ ix_right ] . FPR - fpr ) else df_fpr_tpr . iloc [ ix_right ] ) best = best . to_dict () if metric in REVERSE_DISTANCE_TO_SIMILARITY : best [ \"Threshold\" ] = REVERSE_DISTANCE_TO_SIMILARITY . get ( metric )( best [ \"Threshold\" ] ) threshold_at_fpr [ metric ] = best return threshold_at_fpr run ( self , X , y , metrics = 'cosine_similarity' , same_class_samples = 'full' , different_class_samples = 'minimal' , batch_size = 'best' , shuffle = False , seed = None , return_embeddings = False , p = 3 ) \u00b6 Runs an experiment for face verification Parameters: Name Type Description Default X ndarray Embeddings array required y ndarray Targets for X as integers required metrics Union[str, Sequence[str]] 'cosine_similarity' 'cosine_distance' 'euclidean_distance' 'euclidean_distance_l2' 'minkowski_distance' 'manhattan_distance' 'chebyshev_distance' list/tuple containing more than one of them. 'cosine_similarity' same_class_samples Union[str, int] 'full': Samples all possible images within each class to create all all possible positive pairs. int: Samples specific number of images for every class to create nC2 pairs where n is passed integer. 'full' different_class_samples Union[str, int, Sequence[Union[str, int]]] 'full': Samples one image from every class with all possible pairs of different classes. This can grow exponentially as the number of images increase. (N, M) = (1, \"full\") 'minimal': Samples one image from every class with one image of all other classes. (N, M) = (1, 1). (Default) int: Samples one image from every class with provided number of images of every other class. tuple or list: (N, M) Samples N images from every class with M images of every other class. 'minimal' batch_size Union[str, int] 'best': Let the program decide based on available memory such that every batch will fit into the available memory. (Default) int: Manually decide the batch_size. 'best' shuffle bool Whether to shuffle the returned experiment dataframe. Default: False. False return_embeddings bool Whether to return the embeddings instead of indexes. Default: False False p int The order of the norm of the difference. Should be 0 < p < 1 , Only valid with minkowski_distance as a metric. Default = 3 3 Returns: Type Description pandas.DataFrame A DataFrame representing the experiment results. Exceptions: Type Description ValueError An error occurred with the provided arguments. Notes Same_class_samples If the provided number is greater than the achievable for the class, the maximum possible combinations are used. Different_class_samples If the provided number is greater than the achievable for the class, the maximum possible combinations are used. (N, M) can also be ('full', 'full') but this will calculate all possible combinations between all posibile negative samples. If the dataset is not small this will probably result in an extremely large array!. Source code in evalify/evalify.py def run ( self , X : np . ndarray , y : np . ndarray , metrics : Union [ str , Sequence [ str ]] = \"cosine_similarity\" , same_class_samples : T_str_int = \"full\" , different_class_samples : Union [ str , int , Sequence [ T_str_int ]] = \"minimal\" , batch_size : T_str_int = \"best\" , shuffle : bool = False , seed : int = None , return_embeddings : bool = False , p : int = 3 , ): \"\"\"Runs an experiment for face verification Args: X: Embeddings array y: Targets for X as integers metrics: - 'cosine_similarity' - 'cosine_distance' - 'euclidean_distance' - 'euclidean_distance_l2' - 'minkowski_distance' - 'manhattan_distance' - 'chebyshev_distance' - list/tuple containing more than one of them. same_class_samples: - 'full': Samples all possible images within each class to create all all possible positive pairs. - int: Samples specific number of images for every class to create nC2 pairs where n is passed integer. different_class_samples: - 'full': Samples one image from every class with all possible pairs of different classes. This can grow exponentially as the number of images increase. (N, M) = (1, \"full\") - 'minimal': Samples one image from every class with one image of all other classes. (N, M) = (1, 1). (Default) - int: Samples one image from every class with provided number of images of every other class. - tuple or list: (N, M) Samples N images from every class with M images of every other class. batch_size: - 'best': Let the program decide based on available memory such that every batch will fit into the available memory. (Default) - int: Manually decide the batch_size. shuffle: Whether to shuffle the returned experiment dataframe. Default: False. return_embeddings: Whether to return the embeddings instead of indexes. Default: False p: The order of the norm of the difference. Should be `0 < p < 1`, Only valid with minkowski_distance as a metric. Default = 3 Returns: pandas.DataFrame: A DataFrame representing the experiment results. Raises: ValueError: An error occurred with the provided arguments. Notes: same_class_samples: If the provided number is greater than the achievable for the class, the maximum possible combinations are used. different_class_samples: If the provided number is greater than the achievable for the class, the maximum possible combinations are used. (N, M) can also be ('full', 'full') but this will calculate all possible combinations between all posibile negative samples. If the dataset is not small this will probably result in an extremely large array!. \"\"\" if isinstance ( metrics , str ): metrics = ( metrics ,) self . _validate_args ( metrics , same_class_samples , different_class_samples , batch_size , p ) X , y = _validate_vectors ( X , y ) all_targets = np . unique ( y ) all_pairs = [] metric_fns = list ( map ( metrics_caller . get , metrics )) self . seed = seed self . rng = np . random . default_rng ( self . seed ) for target in all_targets : all_pairs += self . get_pairs ( y , same_class_samples , different_class_samples , target , ) self . df = pd . DataFrame ( data = all_pairs , columns = [ \"img_a\" , \"img_b\" , \"target_a\" , \"target_b\" , \"target\" ] ) if shuffle : self . df = self . df . sample ( frac = 1 , random_state = seed ) if batch_size == \"best\" : batch_size = calculate_best_batch_size ( X ) kwargs = {} if any ( metric in METRICS_NEED_NORM for metric in metrics ): kwargs [ \"norms\" ] = get_norms ( X ) if any ( metric in METRICS_NEED_ORDER for metric in metrics ): kwargs [ \"p\" ] = p img_a = self . df . img_a . to_numpy () img_b = self . df . img_b . to_numpy () experiment_size = len ( self . df ) img_a_s = np . array_split ( img_a , np . ceil ( experiment_size / batch_size )) img_b_s = np . array_split ( img_b , np . ceil ( experiment_size / batch_size )) for metric , metric_fn in zip ( metrics , metric_fns ): self . df [ metric ] = np . hstack ( [ metric_fn ( X , i , j , ** kwargs ) for i , j in zip ( img_a_s , img_b_s )] ) if return_embeddings : self . df [ \"img_a\" ] = X [ img_a ] . tolist () self . df [ \"img_b\" ] = X [ img_b ] . tolist () self . experiment_sucess = True self . metrics = metrics return self . df metrics \u00b6 Evalify metrics module used for calculating the evaluation metrics. Optimized calculations using einstein sum. Embeddings array and norm arrays are indexed with every split and calculations happens over large data chunks very quickly. utils \u00b6 Evalify utils module contains various utilites serving other modules. calculate_best_batch_size ( X , available_mem = None ) \u00b6 Calculate maximum rows to fetch per batch without going out of memory. We need 3 big arrays to be held in memory (A, B, A*B) Source code in evalify/utils.py def calculate_best_batch_size ( X , available_mem = None ): \"\"\"Calculate maximum rows to fetch per batch without going out of memory. We need 3 big arrays to be held in memory (A, B, A*B) \"\"\" available_mem = _calc_available_memory () if available_mem is None else available_mem if available_mem > 2 * GB_TO_BYTE : max_total_rows = np . floor ( available_mem - GB_TO_BYTE / X [ 0 ] . nbytes ) return max_total_rows // 3 else : max_total_rows = np . floor ( available_mem / X [ 0 ] . nbytes ) return max_total_rows // 5","title":"modules"},{"location":"api/#evalify.evalify","text":"Evalify main module used for creating the verification experiments. Creates experiments with embedding pairs to compare for face verification tasks including positive pairs, negative pairs and metrics calculations using a very optimized einstein sum. Many operations are dispatched to canonical BLAS, cuBLAS, or other specialized routines. Extremely large arrays are split into smaller batches, every batch would consume the roughly the maximum available memory. Typical usage example: 1 2 experiment = Experiment() experiment.run(X, y)","title":"evalify"},{"location":"api/#evalify.evalify.Experiment","text":"Source code in evalify/evalify.py class Experiment : def __init__ ( self ) -> None : self . experiment_sucess = False self . cached_predicted_as_similarity = {} def __call__ ( self , * args : Any , ** kwds : Any ) -> Any : return self . run ( * args , ** kwds ) def run ( self , X : np . ndarray , y : np . ndarray , metrics : Union [ str , Sequence [ str ]] = \"cosine_similarity\" , same_class_samples : T_str_int = \"full\" , different_class_samples : Union [ str , int , Sequence [ T_str_int ]] = \"minimal\" , batch_size : T_str_int = \"best\" , shuffle : bool = False , seed : int = None , return_embeddings : bool = False , p : int = 3 , ): \"\"\"Runs an experiment for face verification Args: X: Embeddings array y: Targets for X as integers metrics: - 'cosine_similarity' - 'cosine_distance' - 'euclidean_distance' - 'euclidean_distance_l2' - 'minkowski_distance' - 'manhattan_distance' - 'chebyshev_distance' - list/tuple containing more than one of them. same_class_samples: - 'full': Samples all possible images within each class to create all all possible positive pairs. - int: Samples specific number of images for every class to create nC2 pairs where n is passed integer. different_class_samples: - 'full': Samples one image from every class with all possible pairs of different classes. This can grow exponentially as the number of images increase. (N, M) = (1, \"full\") - 'minimal': Samples one image from every class with one image of all other classes. (N, M) = (1, 1). (Default) - int: Samples one image from every class with provided number of images of every other class. - tuple or list: (N, M) Samples N images from every class with M images of every other class. batch_size: - 'best': Let the program decide based on available memory such that every batch will fit into the available memory. (Default) - int: Manually decide the batch_size. shuffle: Whether to shuffle the returned experiment dataframe. Default: False. return_embeddings: Whether to return the embeddings instead of indexes. Default: False p: The order of the norm of the difference. Should be `0 < p < 1`, Only valid with minkowski_distance as a metric. Default = 3 Returns: pandas.DataFrame: A DataFrame representing the experiment results. Raises: ValueError: An error occurred with the provided arguments. Notes: same_class_samples: If the provided number is greater than the achievable for the class, the maximum possible combinations are used. different_class_samples: If the provided number is greater than the achievable for the class, the maximum possible combinations are used. (N, M) can also be ('full', 'full') but this will calculate all possible combinations between all posibile negative samples. If the dataset is not small this will probably result in an extremely large array!. \"\"\" if isinstance ( metrics , str ): metrics = ( metrics ,) self . _validate_args ( metrics , same_class_samples , different_class_samples , batch_size , p ) X , y = _validate_vectors ( X , y ) all_targets = np . unique ( y ) all_pairs = [] metric_fns = list ( map ( metrics_caller . get , metrics )) self . seed = seed self . rng = np . random . default_rng ( self . seed ) for target in all_targets : all_pairs += self . get_pairs ( y , same_class_samples , different_class_samples , target , ) self . df = pd . DataFrame ( data = all_pairs , columns = [ \"img_a\" , \"img_b\" , \"target_a\" , \"target_b\" , \"target\" ] ) if shuffle : self . df = self . df . sample ( frac = 1 , random_state = seed ) if batch_size == \"best\" : batch_size = calculate_best_batch_size ( X ) kwargs = {} if any ( metric in METRICS_NEED_NORM for metric in metrics ): kwargs [ \"norms\" ] = get_norms ( X ) if any ( metric in METRICS_NEED_ORDER for metric in metrics ): kwargs [ \"p\" ] = p img_a = self . df . img_a . to_numpy () img_b = self . df . img_b . to_numpy () experiment_size = len ( self . df ) img_a_s = np . array_split ( img_a , np . ceil ( experiment_size / batch_size )) img_b_s = np . array_split ( img_b , np . ceil ( experiment_size / batch_size )) for metric , metric_fn in zip ( metrics , metric_fns ): self . df [ metric ] = np . hstack ( [ metric_fn ( X , i , j , ** kwargs ) for i , j in zip ( img_a_s , img_b_s )] ) if return_embeddings : self . df [ \"img_a\" ] = X [ img_a ] . tolist () self . df [ \"img_b\" ] = X [ img_b ] . tolist () self . experiment_sucess = True self . metrics = metrics return self . df def get_pairs ( self , y , same_class_samples , different_class_samples , target , ): same_ixs_full = np . argwhere ( y == target ) . ravel () if isinstance ( same_class_samples , int ): same_class_samples = min ( len ( same_ixs_full ), same_class_samples ) same_ixs = self . rng . choice ( same_ixs_full , same_class_samples ) elif same_class_samples == \"full\" : same_ixs = same_ixs_full same_pairs = itertools . combinations ( same_ixs , 2 ) same_pairs = [( a , b , target , target , 1 ) for a , b in same_pairs ] different_ixs = np . argwhere ( y != target ) . ravel () diff_df = pd . DataFrame ( data = { \"ix\" : different_ixs , \"target\" : y [ different_ixs ]}) diff_df = diff_df . sample ( frac = 1 , random_state = self . seed ) if different_class_samples in [ \"full\" , \"minimal\" ] or isinstance ( different_class_samples , int ): N = 1 if different_class_samples == \"minimal\" : diff_df = diff_df . drop_duplicates ( subset = [ \"target\" ]) else : N , M = different_class_samples N = len ( same_ixs_full ) if N == \"full\" else min ( N , len ( same_ixs_full )) if M != \"full\" : diff_df = diff_df . groupby ( \"target\" ) . apply ( lambda x : x [: M ]) . droplevel ( 0 ) different_ixs = diff_df . ix . to_numpy () different_pairs = itertools . product ( self . rng . choice ( same_ixs_full , N , replace = False ), different_ixs ) different_pairs = [( a , b , target , y [ b ], 0 ) for a , b in different_pairs if a < b ] return same_pairs + different_pairs def _validate_args ( self , metrics , same_class_samples , different_class_samples , batch_size , p ): if same_class_samples != \"full\" and not isinstance ( same_class_samples , int ): raise ValueError ( \"`same_class_samples` argument must be one of 'full' or an integer \" f \"Received: same_class_samples= { same_class_samples } \" ) if different_class_samples not in ( \"full\" , \"minimal\" ): if not isinstance ( different_class_samples , ( int , list , tuple )): raise ValueError ( \"`different_class_samples` argument must be one of 'full', 'minimal', \" \"an integer, a list or tuple of integers or keyword 'full'.\" f \"Received: different_class_samples= { different_class_samples } .\" ) elif isinstance ( different_class_samples , ( list , tuple )): if ( not ( all ( isinstance ( i , int ) or i == \"full\" for i in different_class_samples ) ) or ( len ( different_class_samples )) != 2 ): raise ValueError ( \"When passing `different_class_samples` as a tuple or list, \" \"elements must be exactly two of integer type or keyword 'full' \" \"(N, M). \" f \"Received: different_class_samples= { different_class_samples } .\" ) if batch_size != \"best\" and not isinstance ( batch_size , int ): raise ValueError ( '`batch_size` argument must be either \"best\" or of type integer ' f \"Received: batch_size= { batch_size } with type { type ( batch_size ) } .\" ) if any ( metric not in metrics_caller for metric in metrics ): raise ValueError ( f \"`metric` argument must be one of { tuple ( metrics_caller . keys ()) } \" f \"Received: metric= { metrics } \" ) if p < 1 : raise ValueError ( f \"`p` must be at least 1. Received: p= { p } \" ) def find_optimal_cutoff ( self ): \"\"\"Find the optimal cutoff point Returns: float: optimal cutoff value \"\"\" self . check_experiment_run () self . optimal_cutoff = {} for metric in self . metrics : fpr , tpr , threshold = roc_curve ( self . df [ \"target\" ], self . df [ metric ]) i = np . arange ( len ( tpr )) roc = pd . DataFrame ( { \"tf\" : pd . Series ( tpr - ( 1 - fpr ), index = i ), \"threshold\" : pd . Series ( threshold , index = i ), } ) roc_t = roc . iloc [( roc . tf - 0 ) . abs () . argsort ()[: 1 ]] self . optimal_cutoff [ metric ] = roc_t [ \"threshold\" ] . item () return self . optimal_cutoff def find_threshold_at_fpr ( self , fpr : float ): \"\"\"Runs an experiment for face verification Args: fpr: False positive rate to find best threshold for. Returns: dict: A dictionary with keys as metrics and values as thresholds. Raises: ValueError: If `fpr` is not between 0 and 1. \"\"\" self . check_experiment_run () if not 0 <= fpr <= 1 : raise ValueError ( \"`fpr` must be between 0 and 1. \" f \"Received wanted_fpr= { fpr } \" ) threshold_at_fpr = {} for metric in self . metrics : predicted = self . predicted_as_similarity ( metric ) FPR , TPR , thresholds = roc_curve ( self . df [ \"target\" ], predicted , drop_intermediate = False ) df_fpr_tpr = pd . DataFrame ({ \"FPR\" : FPR , \"TPR\" : TPR , \"Threshold\" : thresholds }) ix_left = np . searchsorted ( df_fpr_tpr [ \"FPR\" ], fpr , side = \"left\" ) ix_right = np . searchsorted ( df_fpr_tpr [ \"FPR\" ], fpr , side = \"right\" ) if fpr == 0 : best = df_fpr_tpr . iloc [ ix_right ] elif fpr == 1 or ix_left == ix_right : best = df_fpr_tpr . iloc [ ix_left ] else : best = ( df_fpr_tpr . iloc [ ix_left ] if abs ( df_fpr_tpr . iloc [ ix_left ] . FPR - fpr ) < abs ( df_fpr_tpr . iloc [ ix_right ] . FPR - fpr ) else df_fpr_tpr . iloc [ ix_right ] ) best = best . to_dict () if metric in REVERSE_DISTANCE_TO_SIMILARITY : best [ \"Threshold\" ] = REVERSE_DISTANCE_TO_SIMILARITY . get ( metric )( best [ \"Threshold\" ] ) threshold_at_fpr [ metric ] = best return threshold_at_fpr def get_binary_prediction ( self , metric , threshold ): return ( self . df [ metric ] . apply ( lambda x : 1 if x < threshold else 0 ) if metric in DISTANCE_TO_SIMILARITY else self . df [ metric ] . apply ( lambda x : 1 if x > threshold else 0 ) ) def evaluate_at_threshold ( self , threshold : float , metric : str ): \"\"\"Evaluate performance at specific threshold Args: threshold: cut-off threshold. metric: metric to use. Returns: dict: containing all evaluation metrics. \"\"\" self . metrics_evaluation = {} self . check_experiment_run ( metric ) for metric in self . metrics : predicted = self . get_binary_prediction ( metric , threshold ) cm = confusion_matrix ( self . df [ \"target\" ], predicted ) tn , fp , fn , tp = cm . ravel () TPR = tp / ( tp + fn ) # recall / true positive rate TNR = tn / ( tn + fp ) # true negative rate PPV = tp / ( tp + fp ) # precision / positive predicted value NPV = tn / ( tn + fn ) # negative predictive value FPR = fp / ( fp + tn ) # false positive rate FNR = 1 - TPR # false negative rate FDR = 1 - PPV # false discovery rate FOR = 1 - NPV # false omission rate F1 = 2 * ( PPV * TPR ) / ( PPV + TPR ) # LRp = TPR / FPR # positive likelihood ratio (LR+) # LRn = FNR / TNR # negative likelihood ratio (LR+) evaluation = { \"TPR\" : TPR , \"TNR\" : TNR , \"PPV\" : PPV , \"NPV\" : NPV , \"FPR\" : FPR , \"FNR\" : FNR , \"FDR\" : FDR , \"FOR\" : FOR , \"F1\" : F1 , # \"LR+\": LRp, # \"LR-\": LRn, } # self.metrics_evaluation[metric] = evaluation return evaluation def check_experiment_run ( self , metric = None ): caller = sys . _getframe () . f_back . f_code . co_name if not self . experiment_sucess : raise NotImplementedError ( f \"` { caller } ` function can only be run after running \" \"`run_experiment`.\" ) if metric is not None and metric not in self . metrics : raise ValueError ( f \"` { caller } ` function was can only be called with `metric` from \" f \" { self . metrics } which were used while running the experiment\" ) def get_roc_auc ( self ): self . check_experiment_run () self . roc_auc = {} for metric in self . metrics : predicted = self . predicted_as_similarity ( metric ) fpr , tpr , thresholds = roc_curve ( self . df [ \"target\" ], predicted , drop_intermediate = False ) self . roc_auc [ metric ] = auc ( fpr , tpr ) self . roc_auc = OrderedDict ( sorted ( self . roc_auc . items (), key = lambda x : x [ 1 ], reverse = True ) ) return self . roc_auc def predicted_as_similarity ( self , metric ): predicted = self . df [ metric ] if metric in DISTANCE_TO_SIMILARITY : predicted = ( self . cached_predicted_as_similarity [ metric ] if metric in self . cached_predicted_as_similarity else DISTANCE_TO_SIMILARITY . get ( metric )( predicted ) ) return predicted","title":"Experiment"},{"location":"api/#evalify.evalify.Experiment.evaluate_at_threshold","text":"Evaluate performance at specific threshold Parameters: Name Type Description Default threshold float cut-off threshold. required metric str metric to use. required Returns: Type Description dict containing all evaluation metrics. Source code in evalify/evalify.py def evaluate_at_threshold ( self , threshold : float , metric : str ): \"\"\"Evaluate performance at specific threshold Args: threshold: cut-off threshold. metric: metric to use. Returns: dict: containing all evaluation metrics. \"\"\" self . metrics_evaluation = {} self . check_experiment_run ( metric ) for metric in self . metrics : predicted = self . get_binary_prediction ( metric , threshold ) cm = confusion_matrix ( self . df [ \"target\" ], predicted ) tn , fp , fn , tp = cm . ravel () TPR = tp / ( tp + fn ) # recall / true positive rate TNR = tn / ( tn + fp ) # true negative rate PPV = tp / ( tp + fp ) # precision / positive predicted value NPV = tn / ( tn + fn ) # negative predictive value FPR = fp / ( fp + tn ) # false positive rate FNR = 1 - TPR # false negative rate FDR = 1 - PPV # false discovery rate FOR = 1 - NPV # false omission rate F1 = 2 * ( PPV * TPR ) / ( PPV + TPR ) # LRp = TPR / FPR # positive likelihood ratio (LR+) # LRn = FNR / TNR # negative likelihood ratio (LR+) evaluation = { \"TPR\" : TPR , \"TNR\" : TNR , \"PPV\" : PPV , \"NPV\" : NPV , \"FPR\" : FPR , \"FNR\" : FNR , \"FDR\" : FDR , \"FOR\" : FOR , \"F1\" : F1 , # \"LR+\": LRp, # \"LR-\": LRn, } # self.metrics_evaluation[metric] = evaluation return evaluation","title":"evaluate_at_threshold()"},{"location":"api/#evalify.evalify.Experiment.find_optimal_cutoff","text":"Find the optimal cutoff point Returns: Type Description float optimal cutoff value Source code in evalify/evalify.py def find_optimal_cutoff ( self ): \"\"\"Find the optimal cutoff point Returns: float: optimal cutoff value \"\"\" self . check_experiment_run () self . optimal_cutoff = {} for metric in self . metrics : fpr , tpr , threshold = roc_curve ( self . df [ \"target\" ], self . df [ metric ]) i = np . arange ( len ( tpr )) roc = pd . DataFrame ( { \"tf\" : pd . Series ( tpr - ( 1 - fpr ), index = i ), \"threshold\" : pd . Series ( threshold , index = i ), } ) roc_t = roc . iloc [( roc . tf - 0 ) . abs () . argsort ()[: 1 ]] self . optimal_cutoff [ metric ] = roc_t [ \"threshold\" ] . item () return self . optimal_cutoff","title":"find_optimal_cutoff()"},{"location":"api/#evalify.evalify.Experiment.find_threshold_at_fpr","text":"Runs an experiment for face verification Parameters: Name Type Description Default fpr float False positive rate to find best threshold for. required Returns: Type Description dict A dictionary with keys as metrics and values as thresholds. Exceptions: Type Description ValueError If fpr is not between 0 and 1. Source code in evalify/evalify.py def find_threshold_at_fpr ( self , fpr : float ): \"\"\"Runs an experiment for face verification Args: fpr: False positive rate to find best threshold for. Returns: dict: A dictionary with keys as metrics and values as thresholds. Raises: ValueError: If `fpr` is not between 0 and 1. \"\"\" self . check_experiment_run () if not 0 <= fpr <= 1 : raise ValueError ( \"`fpr` must be between 0 and 1. \" f \"Received wanted_fpr= { fpr } \" ) threshold_at_fpr = {} for metric in self . metrics : predicted = self . predicted_as_similarity ( metric ) FPR , TPR , thresholds = roc_curve ( self . df [ \"target\" ], predicted , drop_intermediate = False ) df_fpr_tpr = pd . DataFrame ({ \"FPR\" : FPR , \"TPR\" : TPR , \"Threshold\" : thresholds }) ix_left = np . searchsorted ( df_fpr_tpr [ \"FPR\" ], fpr , side = \"left\" ) ix_right = np . searchsorted ( df_fpr_tpr [ \"FPR\" ], fpr , side = \"right\" ) if fpr == 0 : best = df_fpr_tpr . iloc [ ix_right ] elif fpr == 1 or ix_left == ix_right : best = df_fpr_tpr . iloc [ ix_left ] else : best = ( df_fpr_tpr . iloc [ ix_left ] if abs ( df_fpr_tpr . iloc [ ix_left ] . FPR - fpr ) < abs ( df_fpr_tpr . iloc [ ix_right ] . FPR - fpr ) else df_fpr_tpr . iloc [ ix_right ] ) best = best . to_dict () if metric in REVERSE_DISTANCE_TO_SIMILARITY : best [ \"Threshold\" ] = REVERSE_DISTANCE_TO_SIMILARITY . get ( metric )( best [ \"Threshold\" ] ) threshold_at_fpr [ metric ] = best return threshold_at_fpr","title":"find_threshold_at_fpr()"},{"location":"api/#evalify.evalify.Experiment.run","text":"Runs an experiment for face verification Parameters: Name Type Description Default X ndarray Embeddings array required y ndarray Targets for X as integers required metrics Union[str, Sequence[str]] 'cosine_similarity' 'cosine_distance' 'euclidean_distance' 'euclidean_distance_l2' 'minkowski_distance' 'manhattan_distance' 'chebyshev_distance' list/tuple containing more than one of them. 'cosine_similarity' same_class_samples Union[str, int] 'full': Samples all possible images within each class to create all all possible positive pairs. int: Samples specific number of images for every class to create nC2 pairs where n is passed integer. 'full' different_class_samples Union[str, int, Sequence[Union[str, int]]] 'full': Samples one image from every class with all possible pairs of different classes. This can grow exponentially as the number of images increase. (N, M) = (1, \"full\") 'minimal': Samples one image from every class with one image of all other classes. (N, M) = (1, 1). (Default) int: Samples one image from every class with provided number of images of every other class. tuple or list: (N, M) Samples N images from every class with M images of every other class. 'minimal' batch_size Union[str, int] 'best': Let the program decide based on available memory such that every batch will fit into the available memory. (Default) int: Manually decide the batch_size. 'best' shuffle bool Whether to shuffle the returned experiment dataframe. Default: False. False return_embeddings bool Whether to return the embeddings instead of indexes. Default: False False p int The order of the norm of the difference. Should be 0 < p < 1 , Only valid with minkowski_distance as a metric. Default = 3 3 Returns: Type Description pandas.DataFrame A DataFrame representing the experiment results. Exceptions: Type Description ValueError An error occurred with the provided arguments. Notes Same_class_samples If the provided number is greater than the achievable for the class, the maximum possible combinations are used. Different_class_samples If the provided number is greater than the achievable for the class, the maximum possible combinations are used. (N, M) can also be ('full', 'full') but this will calculate all possible combinations between all posibile negative samples. If the dataset is not small this will probably result in an extremely large array!. Source code in evalify/evalify.py def run ( self , X : np . ndarray , y : np . ndarray , metrics : Union [ str , Sequence [ str ]] = \"cosine_similarity\" , same_class_samples : T_str_int = \"full\" , different_class_samples : Union [ str , int , Sequence [ T_str_int ]] = \"minimal\" , batch_size : T_str_int = \"best\" , shuffle : bool = False , seed : int = None , return_embeddings : bool = False , p : int = 3 , ): \"\"\"Runs an experiment for face verification Args: X: Embeddings array y: Targets for X as integers metrics: - 'cosine_similarity' - 'cosine_distance' - 'euclidean_distance' - 'euclidean_distance_l2' - 'minkowski_distance' - 'manhattan_distance' - 'chebyshev_distance' - list/tuple containing more than one of them. same_class_samples: - 'full': Samples all possible images within each class to create all all possible positive pairs. - int: Samples specific number of images for every class to create nC2 pairs where n is passed integer. different_class_samples: - 'full': Samples one image from every class with all possible pairs of different classes. This can grow exponentially as the number of images increase. (N, M) = (1, \"full\") - 'minimal': Samples one image from every class with one image of all other classes. (N, M) = (1, 1). (Default) - int: Samples one image from every class with provided number of images of every other class. - tuple or list: (N, M) Samples N images from every class with M images of every other class. batch_size: - 'best': Let the program decide based on available memory such that every batch will fit into the available memory. (Default) - int: Manually decide the batch_size. shuffle: Whether to shuffle the returned experiment dataframe. Default: False. return_embeddings: Whether to return the embeddings instead of indexes. Default: False p: The order of the norm of the difference. Should be `0 < p < 1`, Only valid with minkowski_distance as a metric. Default = 3 Returns: pandas.DataFrame: A DataFrame representing the experiment results. Raises: ValueError: An error occurred with the provided arguments. Notes: same_class_samples: If the provided number is greater than the achievable for the class, the maximum possible combinations are used. different_class_samples: If the provided number is greater than the achievable for the class, the maximum possible combinations are used. (N, M) can also be ('full', 'full') but this will calculate all possible combinations between all posibile negative samples. If the dataset is not small this will probably result in an extremely large array!. \"\"\" if isinstance ( metrics , str ): metrics = ( metrics ,) self . _validate_args ( metrics , same_class_samples , different_class_samples , batch_size , p ) X , y = _validate_vectors ( X , y ) all_targets = np . unique ( y ) all_pairs = [] metric_fns = list ( map ( metrics_caller . get , metrics )) self . seed = seed self . rng = np . random . default_rng ( self . seed ) for target in all_targets : all_pairs += self . get_pairs ( y , same_class_samples , different_class_samples , target , ) self . df = pd . DataFrame ( data = all_pairs , columns = [ \"img_a\" , \"img_b\" , \"target_a\" , \"target_b\" , \"target\" ] ) if shuffle : self . df = self . df . sample ( frac = 1 , random_state = seed ) if batch_size == \"best\" : batch_size = calculate_best_batch_size ( X ) kwargs = {} if any ( metric in METRICS_NEED_NORM for metric in metrics ): kwargs [ \"norms\" ] = get_norms ( X ) if any ( metric in METRICS_NEED_ORDER for metric in metrics ): kwargs [ \"p\" ] = p img_a = self . df . img_a . to_numpy () img_b = self . df . img_b . to_numpy () experiment_size = len ( self . df ) img_a_s = np . array_split ( img_a , np . ceil ( experiment_size / batch_size )) img_b_s = np . array_split ( img_b , np . ceil ( experiment_size / batch_size )) for metric , metric_fn in zip ( metrics , metric_fns ): self . df [ metric ] = np . hstack ( [ metric_fn ( X , i , j , ** kwargs ) for i , j in zip ( img_a_s , img_b_s )] ) if return_embeddings : self . df [ \"img_a\" ] = X [ img_a ] . tolist () self . df [ \"img_b\" ] = X [ img_b ] . tolist () self . experiment_sucess = True self . metrics = metrics return self . df","title":"run()"},{"location":"api/#evalify.metrics","text":"Evalify metrics module used for calculating the evaluation metrics. Optimized calculations using einstein sum. Embeddings array and norm arrays are indexed with every split and calculations happens over large data chunks very quickly.","title":"metrics"},{"location":"api/#evalify.utils","text":"Evalify utils module contains various utilites serving other modules.","title":"utils"},{"location":"api/#evalify.utils.calculate_best_batch_size","text":"Calculate maximum rows to fetch per batch without going out of memory. We need 3 big arrays to be held in memory (A, B, A*B) Source code in evalify/utils.py def calculate_best_batch_size ( X , available_mem = None ): \"\"\"Calculate maximum rows to fetch per batch without going out of memory. We need 3 big arrays to be held in memory (A, B, A*B) \"\"\" available_mem = _calc_available_memory () if available_mem is None else available_mem if available_mem > 2 * GB_TO_BYTE : max_total_rows = np . floor ( available_mem - GB_TO_BYTE / X [ 0 ] . nbytes ) return max_total_rows // 3 else : max_total_rows = np . floor ( available_mem / X [ 0 ] . nbytes ) return max_total_rows // 5","title":"calculate_best_batch_size()"},{"location":"authors/","text":"Credits \u00b6 Development Lead \u00b6 Mahmoud Bahaa mah.alaa@nu.edu.eg Contributors \u00b6 None yet. Why not be the first? Others \u00b6 This package was created with Cookiecutter and the zillionare/cookiecutter-pypackage project template. Logo was created using font GlacialIndifference-Regular by Hanken Design Co. Logo icon designed by Mauro Lucchesi","title":"authors"},{"location":"authors/#credits","text":"","title":"Credits"},{"location":"authors/#development-lead","text":"Mahmoud Bahaa mah.alaa@nu.edu.eg","title":"Development Lead"},{"location":"authors/#contributors","text":"None yet. Why not be the first?","title":"Contributors"},{"location":"authors/#others","text":"This package was created with Cookiecutter and the zillionare/cookiecutter-pypackage project template. Logo was created using font GlacialIndifference-Regular by Hanken Design Co. Logo icon designed by Mauro Lucchesi","title":"Others"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcomed, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/ma7555/evalify/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation \u00b6 evalify could always use more documentation, whether as part of the official evalify docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/ma7555/evalify/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up evalify for local development. Fork the evalify repo on GitHub. Clone your fork locally 1 git clone git@github.com:your_name_here/evalify.git Ensure poetry is installed. Install dependencies and start your virtualenv: 1 poetry install -E test -E doc -E dev Create a branch for local development: 1 git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: 1 tox Commit your changes and push your branch to GitHub: 1 2 3 git add . git commit -m \"Your detailed description of your changes.\" git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.6, 3.7, 3.8, 3.9 and for PyPy. Check https://github.com/ma7555/evalify/actions and make sure that the tests pass for all supported Python versions. \u00b6 1 python -m unittest or 1 pytest To run a subset of tests. Deploying \u00b6 A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in HISTORY.md). Then run: 1 2 git push git push --tags Github Actions will then deploy to PyPI if tests pass.","title":"contributing"},{"location":"contributing/#contributing","text":"Contributions are welcomed, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/ma7555/evalify/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"evalify could always use more documentation, whether as part of the official evalify docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/ma7555/evalify/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up evalify for local development. Fork the evalify repo on GitHub. Clone your fork locally 1 git clone git@github.com:your_name_here/evalify.git Ensure poetry is installed. Install dependencies and start your virtualenv: 1 poetry install -E test -E doc -E dev Create a branch for local development: 1 git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: 1 tox Commit your changes and push your branch to GitHub: 1 2 3 git add . git commit -m \"Your detailed description of your changes.\" git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.6, 3.7, 3.8, 3.9 and for PyPy. Check https://github.com/ma7555/evalify/actions and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"contributing/#_1","text":"1 python -m unittest or 1 pytest To run a subset of tests.","title":""},{"location":"contributing/#deploying","text":"A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in HISTORY.md). Then run: 1 2 git push git push --tags Github Actions will then deploy to PyPI if tests pass.","title":"Deploying"},{"location":"history/","text":"History \u00b6 0.1.0 (2022-02-20) \u00b6 First release on PyPI. 0.1.1 (2022-02-22) \u00b6 Run time enhancement.","title":"history"},{"location":"history/#history","text":"","title":"History"},{"location":"history/#010-2022-02-20","text":"First release on PyPI.","title":"0.1.0 (2022-02-20)"},{"location":"history/#011-2022-02-22","text":"Run time enhancement.","title":"0.1.1 (2022-02-22)"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install evalify, run this command in your terminal: 1 pip install evalify This is the preferred method to install evalify, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From source \u00b6 The source for evalify can be downloaded from the Github repo . You can either clone the public repository: 1 git clone git://github.com/ma7555/evalify Or download the tarball : 1 curl -OJL https://github.com/ma7555/evalify/tarball/master Once you have a copy of the source, you can install it with: 1 pip install .","title":"installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install evalify, run this command in your terminal: 1 pip install evalify This is the preferred method to install evalify, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-source","text":"The source for evalify can be downloaded from the Github repo . You can either clone the public repository: 1 git clone git://github.com/ma7555/evalify Or download the tarball : 1 curl -OJL https://github.com/ma7555/evalify/tarball/master Once you have a copy of the source, you can install it with: 1 pip install .","title":"From source"},{"location":"usage/","text":"Usage \u00b6 To use evalify in a project 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import numpy as np from evalify import Experiment rng = np . random . default_rng () nphotos = 500 emb_size = 32 nclasses = 10 X = rng . random (( self . nphotos , self . emb_size )) y = rng . integers ( self . nclasses , size = self . nphotos ) experiment = Experiment () experiment . run ( X , y ) experiment . get_roc_auc () print ( experiment . df . roc_auc ) For a working experiment using real face embeddings, please refer to LFW.py under ./examples . 1 python ./ examples / LFW . py 1 2 3 4 Total available embeddings 2921 resulted in 4264660 samples for the experiment. Metrics calculations executed in 24.05 seconds ROC AUC: OrderedDict([('euclidean_distance', 0.9991302819624498), ('cosine_distance', 0.9991302818953706), ('euclidean_distance_l2', 0.9991302818953706), ('manhattan_distance', 0.9991260462584446)])","title":"usage"},{"location":"usage/#usage","text":"To use evalify in a project 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import numpy as np from evalify import Experiment rng = np . random . default_rng () nphotos = 500 emb_size = 32 nclasses = 10 X = rng . random (( self . nphotos , self . emb_size )) y = rng . integers ( self . nclasses , size = self . nphotos ) experiment = Experiment () experiment . run ( X , y ) experiment . get_roc_auc () print ( experiment . df . roc_auc ) For a working experiment using real face embeddings, please refer to LFW.py under ./examples . 1 python ./ examples / LFW . py 1 2 3 4 Total available embeddings 2921 resulted in 4264660 samples for the experiment. Metrics calculations executed in 24.05 seconds ROC AUC: OrderedDict([('euclidean_distance', 0.9991302819624498), ('cosine_distance', 0.9991302818953706), ('euclidean_distance_l2', 0.9991302818953706), ('manhattan_distance', 0.9991260462584446)])","title":"Usage"}]}